[Script Info]
; Script generated by Aegisub 3.2.2
; http://www.aegisub.org/
Title: Default Aegisub file
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
YCbCr Matrix: TV.601
PlayResX: 1280
PlayResY: 720

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: ENG,Inter,36,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,1.5,1.5,2,6,6,6,1
Style: CHN,MonyharOS Sans SC Medium,42,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,12,12,6,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:00.00,0:00:01.35,ENG,,0,0,0,,- For hundreds of years,
Dialogue: 0,0:00:00.00,0:00:01.35,CHN,,0,0,0,,几百年来
Dialogue: 0,0:00:01.35,0:00:05.10,ENG,,0,0,0,,analog computers were the most powerful computers on Earth,
Dialogue: 0,0:00:01.35,0:00:05.10,CHN,,0,0,0,,模拟计算机是世界上最强大的计算机
Dialogue: 0,0:00:05.10,0:00:09.72,ENG,,0,0,0,,predicting eclipses, tides, and guiding anti-aircraft guns.
Dialogue: 0,0:00:05.10,0:00:09.72,CHN,,0,0,0,,被用来预测月食、潮汐和引导防空武器
Dialogue: 0,0:00:09.72,0:00:12.65,ENG,,0,0,0,,Then, with the advent of solid-state transistors,
Dialogue: 0,0:00:09.72,0:00:12.65,CHN,,0,0,0,,随着固态晶体管的出现
Dialogue: 0,0:00:12.65,0:00:14.50,ENG,,0,0,0,,digital computers took off.
Dialogue: 0,0:00:12.65,0:00:14.50,CHN,,0,0,0,,电子计算机蓬勃发展
Dialogue: 0,0:00:14.50,0:00:18.08,ENG,,0,0,0,,Now, virtually every computer we use is digital.
Dialogue: 0,0:00:14.50,0:00:18.08,CHN,,0,0,0,,现如今  几乎所有计算机都是电子的
Dialogue: 0,0:00:18.08,0:00:21.73,ENG,,0,0,0,,But today, a perfect storm of factors is setting the scene
Dialogue: 0,0:00:18.08,0:00:21.73,CHN,,0,0,0,,但是现在又有一波新趋势
Dialogue: 0,0:00:21.73,0:00:24.90,ENG,,0,0,0,,for a resurgence of analog technology.
Dialogue: 0,0:00:21.73,0:00:24.90,CHN,,0,0,0,,推动着模拟技术的复苏
Dialogue: 0,0:00:24.90,0:00:27.53,ENG,,0,0,0,,This is an analog computer,
Dialogue: 0,0:00:24.90,0:00:27.53,CHN,,0,0,0,,这是一台模拟计算机
Dialogue: 0,0:00:27.53,0:00:30.43,ENG,,0,0,0,,and by connecting these wires in particular ways,
Dialogue: 0,0:00:27.53,0:00:30.43,CHN,,0,0,0,,我可以通过特定的接线方式
Dialogue: 0,0:00:30.43,0:00:32.67,ENG,,0,0,0,,I can program it to solve a whole range
Dialogue: 0,0:00:30.43,0:00:32.67,CHN,,0,0,0,,给它编写程序
Dialogue: 0,0:00:32.67,0:00:34.87,ENG,,0,0,0,,of differential equations.
Dialogue: 0,0:00:32.67,0:00:34.87,CHN,,0,0,0,,让它解不同的微分方程
Dialogue: 0,0:00:34.87,0:00:37.74,ENG,,0,0,0,,For example, this setup allows me to simulate
Dialogue: 0,0:00:37.74,0:00:40.80,ENG,,0,0,0,,a damped mass oscillating on a spring.
Dialogue: 0,0:00:34.87,0:00:40.80,CHN,,0,0,0,,例如 我这样接线 就可以模拟一个带阻尼的重物在弹簧上振动
Dialogue: 0,0:00:37.74,0:00:40.80,CHN,,0,0,0,,
Dialogue: 0,0:00:40.80,0:00:43.06,ENG,,0,0,0,,So on the oscilloscope you can actually see
Dialogue: 0,0:00:43.06,0:00:45.75,ENG,,0,0,0,,the position of the mass over time.
Dialogue: 0,0:00:40.80,0:00:45.75,CHN,,0,0,0,,然后在示波器上 就能看到重物的位移随时间变化的图像
Dialogue: 0,0:00:43.06,0:00:45.75,CHN,,0,0,0,,
Dialogue: 0,0:00:45.75,0:00:48.94,ENG,,0,0,0,,And I can vary the damping,
Dialogue: 0,0:00:45.75,0:00:48.94,CHN,,0,0,0,,我还可以改变阻尼
Dialogue: 0,0:00:48.94,0:00:51.90,ENG,,0,0,0,,or the spring constant,
Dialogue: 0,0:00:48.94,0:00:51.90,CHN,,0,0,0,,或者弹簧的弹力系数
Dialogue: 0,0:00:51.90,0:00:53.81,ENG,,0,0,0,,or the mass, and we can see
Dialogue: 0,0:00:51.90,0:00:53.81,CHN,,0,0,0,,又或者重物的质量
Dialogue: 0,0:00:53.81,0:00:57.74,ENG,,0,0,0,,how the amplitude and duration of the oscillations change.
Dialogue: 0,0:00:53.81,0:00:57.74,CHN,,0,0,0,,就可以看到振幅和振动时长如何改变
Dialogue: 0,0:00:57.74,0:01:00.06,ENG,,0,0,0,,Now what makes this an analog computer
Dialogue: 0,0:00:57.74,0:01:00.06,CHN,,0,0,0,,而它之所以被称作模拟计算机
Dialogue: 0,0:01:00.06,0:01:03.44,ENG,,0,0,0,,is that there are no zeros and ones in here.
Dialogue: 0,0:01:00.06,0:01:03.44,CHN,,0,0,0,,是因为在它的概念里没有 0 和 1
Dialogue: 0,0:01:03.44,0:01:06.81,ENG,,0,0,0,,Instead, there's actually a voltage that oscillates
Dialogue: 0,0:01:03.44,0:01:06.81,CHN,,0,0,0,,而是真正的有一个电压
Dialogue: 0,0:01:06.81,0:01:10.26,ENG,,0,0,0,,up and down exactly like a mass on a spring.
Dialogue: 0,0:01:06.81,0:01:10.26,CHN,,0,0,0,,像重物在弹簧上一样上下振动
Dialogue: 0,0:01:10.26,0:01:14.33,ENG,,0,0,0,,The electrical circuitry is an analog for the physical problem,
Dialogue: 0,0:01:10.26,0:01:14.33,CHN,,0,0,0,,这个电子电路模拟了这个物理问题
Dialogue: 0,0:01:14.33,0:01:16.66,ENG,,0,0,0,,it just takes place much faster.
Dialogue: 0,0:01:14.33,0:01:16.66,CHN,,0,0,0,,使得运算快得多
Dialogue: 0,0:01:16.66,0:01:19.12,ENG,,0,0,0,,Now, if I change the electrical connections,
Dialogue: 0,0:01:16.66,0:01:19.12,CHN,,0,0,0,,现在如果我换一组接线
Dialogue: 0,0:01:19.12,0:01:20.30,ENG,,0,0,0,,I can program this computer
Dialogue: 0,0:01:19.12,0:01:20.30,CHN,,0,0,0,,就可以重新编程
Dialogue: 0,0:01:20.30,0:01:22.27,ENG,,0,0,0,,to solve other differential equations,
Dialogue: 0,0:01:20.30,0:01:22.27,CHN,,0,0,0,,算其他的微分方程
Dialogue: 0,0:01:22.27,0:01:24.01,ENG,,0,0,0,,like the Lorenz system,
Dialogue: 0,0:01:22.27,0:01:24.01,CHN,,0,0,0,,例如洛伦茨振子
Dialogue: 0,0:01:24.01,0:01:27.29,ENG,,0,0,0,,which is a basic model of convection in the atmosphere.
Dialogue: 0,0:01:24.01,0:01:27.29,CHN,,0,0,0,,这是大气对流的基本模型
Dialogue: 0,0:01:27.29,0:01:29.05,ENG,,0,0,0,,Now the Lorenz system is famous because
Dialogue: 0,0:01:29.05,0:01:32.27,ENG,,0,0,0,,it was one of the first discovered examples of chaos.
Dialogue: 0,0:01:27.29,0:01:32.27,CHN,,0,0,0,,它作为最早发现的混沌系统而闻名
Dialogue: 0,0:01:29.05,0:01:32.27,CHN,,0,0,0,,
Dialogue: 0,0:01:32.27,0:01:35.52,ENG,,0,0,0,,And here, you can see the Lorenz attractor
Dialogue: 0,0:01:32.27,0:01:35.52,CHN,,0,0,0,,在屏幕上 你可以看到
Dialogue: 0,0:01:35.52,0:01:38.40,ENG,,0,0,0,,with its beautiful butterfly shape.
Dialogue: 0,0:01:35.52,0:01:38.40,CHN,,0,0,0,,洛伦茨吸引子美妙的蝴蝶图样
Dialogue: 0,0:01:38.40,0:01:39.90,ENG,,0,0,0,,And on this analog computer,
Dialogue: 0,0:01:38.40,0:01:39.90,CHN,,0,0,0,,在这台模拟计算机上
Dialogue: 0,0:01:39.90,0:01:42.36,ENG,,0,0,0,,I can change the parameters
Dialogue: 0,0:01:39.90,0:01:42.36,CHN,,0,0,0,,我可以实时地看到
Dialogue: 0,0:01:42.36,0:01:45.56,ENG,,0,0,0,,and see their effects in real time.
Dialogue: 0,0:01:42.36,0:01:45.56,CHN,,0,0,0,,我改变参数的效果
Dialogue: 0,0:01:46.41,0:01:47.98,ENG,,0,0,0,,So these examples illustrate some
Dialogue: 0,0:01:46.41,0:01:47.98,CHN,,0,0,0,,这几个例子展示了
Dialogue: 0,0:01:47.98,0:01:50.65,ENG,,0,0,0,,of the advantages of analog computers.
Dialogue: 0,0:01:47.98,0:01:50.65,CHN,,0,0,0,,模拟计算机的优势
Dialogue: 0,0:01:50.65,0:01:53.26,ENG,,0,0,0,,They are incredibly powerful computing devices,
Dialogue: 0,0:01:50.65,0:01:53.26,CHN,,0,0,0,,它十分强大
Dialogue: 0,0:01:53.26,0:01:56.58,ENG,,0,0,0,,and they can complete a lot of computations fast.
Dialogue: 0,0:01:53.26,0:01:56.58,CHN,,0,0,0,,运行得特别快
Dialogue: 0,0:01:56.58,0:01:59.13,ENG,,0,0,0,,Plus, they don't take much power to do it.
Dialogue: 0,0:01:56.58,0:01:59.13,CHN,,0,0,0,,而且 它还非常省电
Dialogue: 0,0:02:01.52,0:02:02.90,ENG,,0,0,0,,With a digital computer,
Dialogue: 0,0:02:01.52,0:02:02.90,CHN,,0,0,0,,在电子计算机里
Dialogue: 0,0:02:02.90,0:02:05.60,ENG,,0,0,0,,if you wanna add two eight-bit numbers,
Dialogue: 0,0:02:02.90,0:02:05.60,CHN,,0,0,0,,把两个八位二进制数相加
Dialogue: 0,0:02:05.60,0:02:08.12,ENG,,0,0,0,,you need around 50 transistors,
Dialogue: 0,0:02:05.60,0:02:08.12,CHN,,0,0,0,,需要大概 50 个晶体管
Dialogue: 0,0:02:08.12,0:02:09.76,ENG,,0,0,0,,whereas with an analog computer,
Dialogue: 0,0:02:08.12,0:02:09.76,CHN,,0,0,0,,而在模拟计算机里
Dialogue: 0,0:02:09.76,0:02:12.24,ENG,,0,0,0,,you can add two currents,
Dialogue: 0,0:02:09.76,0:02:12.24,CHN,,0,0,0,,只要连两根线
Dialogue: 0,0:02:12.24,0:02:15.76,ENG,,0,0,0,,just by connecting two wires.
Dialogue: 0,0:02:12.24,0:02:15.76,CHN,,0,0,0,,就能把两个电流相加
Dialogue: 0,0:02:15.76,0:02:18.38,ENG,,0,0,0,,With a digital computer to multiply two numbers,
Dialogue: 0,0:02:15.76,0:02:18.38,CHN,,0,0,0,,而用电子计算机计算两个数的乘积
Dialogue: 0,0:02:18.38,0:02:20.97,ENG,,0,0,0,,you need on the order of 1,000 transistors
Dialogue: 0,0:02:18.38,0:02:20.97,CHN,,0,0,0,,需要数以千计的晶体管
Dialogue: 0,0:02:20.97,0:02:23.60,ENG,,0,0,0,,all switching zeros and ones,
Dialogue: 0,0:02:20.97,0:02:23.60,CHN,,0,0,0,,在 0 和 1 之间反复横跳
Dialogue: 0,0:02:23.60,0:02:24.96,ENG,,0,0,0,,whereas with an analog computer,
Dialogue: 0,0:02:23.60,0:02:24.96,CHN,,0,0,0,,而在模拟计算机里
Dialogue: 0,0:02:24.96,0:02:28.48,ENG,,0,0,0,,you can pass a current through a resistor,
Dialogue: 0,0:02:24.96,0:02:28.48,CHN,,0,0,0,,只需要让电流流过一个电阻
Dialogue: 0,0:02:28.48,0:02:31.85,ENG,,0,0,0,,and then the voltage across this resistor
Dialogue: 0,0:02:28.48,0:02:31.85,CHN,,0,0,0,,这样流过这个电阻两端的电压
Dialogue: 0,0:02:31.85,0:02:34.29,ENG,,0,0,0,,will be I times R.
Dialogue: 0,0:02:31.85,0:02:34.29,CHN,,0,0,0,,就是 I*R 了
Dialogue: 0,0:02:34.29,0:02:35.70,ENG,,0,0,0,,So effectively,
Dialogue: 0,0:02:34.29,0:02:35.70,CHN,,0,0,0,,这就等效于
Dialogue: 0,0:02:35.70,0:02:38.88,ENG,,0,0,0,,you have multiplied two numbers together.
Dialogue: 0,0:02:35.70,0:02:38.88,CHN,,0,0,0,,你把两个数值做了乘法
Dialogue: 0,0:02:40.01,0:02:42.93,ENG,,0,0,0,,But analog computers also have their drawbacks.
Dialogue: 0,0:02:40.01,0:02:42.93,CHN,,0,0,0,,但是模拟计算机也有缺点
Dialogue: 0,0:02:42.93,0:02:43.76,ENG,,0,0,0,,For one thing,
Dialogue: 0,0:02:42.93,0:02:43.76,CHN,,0,0,0,,首先
Dialogue: 0,0:02:43.76,0:02:46.43,ENG,,0,0,0,,they are not general-purpose computing devices.
Dialogue: 0,0:02:43.76,0:02:46.43,CHN,,0,0,0,,它不是通用计算设备
Dialogue: 0,0:02:46.43,0:02:49.43,ENG,,0,0,0,,I mean, you're not gonna run Microsoft Word on this thing.
Dialogue: 0,0:02:46.43,0:02:49.43,CHN,,0,0,0,,在上面没法运行 Word
Dialogue: 0,0:02:49.43,0:02:52.99,ENG,,0,0,0,,And also, since the inputs and outputs are continuous,
Dialogue: 0,0:02:49.43,0:02:52.99,CHN,,0,0,0,,并且由于它的输入是连续的
Dialogue: 0,0:02:52.99,0:02:55.92,ENG,,0,0,0,,I can't input exact values.
Dialogue: 0,0:02:52.99,0:02:55.92,CHN,,0,0,0,,我并不能输入准确值
Dialogue: 0,0:02:55.92,0:02:58.98,ENG,,0,0,0,,So if I try to repeat the same calculation,
Dialogue: 0,0:02:55.92,0:02:58.98,CHN,,0,0,0,,因此重复相同的运算
Dialogue: 0,0:02:58.98,0:03:01.87,ENG,,0,0,0,,I'm never going to get the exact same answer.
Dialogue: 0,0:02:58.98,0:03:01.87,CHN,,0,0,0,,并不能得到相同的结果
Dialogue: 0,0:03:01.87,0:03:04.66,ENG,,0,0,0,,Plus, think about manufacturing analog computers.
Dialogue: 0,0:03:01.87,0:03:04.66,CHN,,0,0,0,,此外 在模拟计算机的制造过程中
Dialogue: 0,0:03:04.66,0:03:06.30,ENG,,0,0,0,,There's always gonna be some variation
Dialogue: 0,0:03:04.66,0:03:10.37,CHN,,0,0,0,,电阻、电容等元件的实际参数总是有所浮动
Dialogue: 0,0:03:06.30,0:03:08.20,ENG,,0,0,0,,in the exact value of components,
Dialogue: 0,0:03:06.30,0:03:08.20,CHN,,0,0,0,,
Dialogue: 0,0:03:08.20,0:03:10.37,ENG,,0,0,0,,like resistors or capacitors.
Dialogue: 0,0:03:08.20,0:03:10.37,CHN,,0,0,0,,
Dialogue: 0,0:03:10.37,0:03:12.26,ENG,,0,0,0,,So as a general rule of thumb,
Dialogue: 0,0:03:10.37,0:03:12.26,CHN,,0,0,0,,因此 一般来说
Dialogue: 0,0:03:12.26,0:03:15.54,ENG,,0,0,0,,you can expect about a 1% error.
Dialogue: 0,0:03:12.26,0:03:15.54,CHN,,0,0,0,,它大概会有 1% 的误差
Dialogue: 0,0:03:15.54,0:03:17.34,ENG,,0,0,0,,So when you think of analog computers,
Dialogue: 0,0:03:15.54,0:03:17.34,CHN,,0,0,0,,因此总结一下
Dialogue: 0,0:03:17.34,0:03:20.82,ENG,,0,0,0,,you can think powerful, fast, and energy-efficient,
Dialogue: 0,0:03:17.34,0:03:20.82,CHN,,0,0,0,,模拟计算机很强大 跑得很快 很省电
Dialogue: 0,0:03:20.82,0:03:25.73,ENG,,0,0,0,,but also single-purpose, non-repeatable, and inexact.
Dialogue: 0,0:03:20.82,0:03:25.73,CHN,,0,0,0,,但也有着用途单一、不可重复、不精确的缺点
Dialogue: 0,0:03:25.73,0:03:28.14,ENG,,0,0,0,,And if those sound like deal-breakers,
Dialogue: 0,0:03:25.73,0:03:28.14,CHN,,0,0,0,,如果这些东西听上去很差劲
Dialogue: 0,0:03:28.14,0:03:30.01,ENG,,0,0,0,,it's because they probably are.
Dialogue: 0,0:03:28.14,0:03:30.01,CHN,,0,0,0,,那么你是对的
Dialogue: 0,0:03:30.01,0:03:31.85,ENG,,0,0,0,,I think these are the major reasons
Dialogue: 0,0:03:30.01,0:03:31.85,CHN,,0,0,0,,我认为  以上这些理由
Dialogue: 0,0:03:31.85,0:03:33.61,ENG,,0,0,0,,why analog computers fell out of favor
Dialogue: 0,0:03:33.61,0:03:36.90,ENG,,0,0,0,,as soon as digital computers became viable.
Dialogue: 0,0:03:31.85,0:03:33.61,CHN,,0,0,0,,让电子计算机一出现
Dialogue: 0,0:03:33.61,0:03:36.90,CHN,,0,0,0,,就取代了模拟计算机
Dialogue: 0,0:03:36.90,0:03:41.31,ENG,,0,0,0,,Now, here's why analog computers may be making a comeback.
Dialogue: 0,0:03:36.90,0:03:41.31,CHN,,0,0,0,,现在我们来说说为什么模拟计算机可能重获青睐
Dialogue: 0,0:03:43.80,0:03:46.50,ENG,,0,0,0,,It all starts with artificial intelligence.
Dialogue: 0,0:03:43.80,0:03:46.50,CHN,,0,0,0,,这一切都源于人工智能的出现
Dialogue: 0,0:03:46.50,0:03:50.04,ENG,,0,0,0,,- A machine has been programmed to see and to move objects.
Dialogue: 0,0:03:46.50,0:03:50.04,CHN,,0,0,0,,这台机器经过编程  能看到物体、移动物体
Dialogue: 0,0:03:51.20,0:03:52.94,ENG,,0,0,0,,- AI isn't new.
Dialogue: 0,0:03:51.20,0:03:52.94,CHN,,0,0,0,,AI 并非新鲜事物
Dialogue: 0,0:03:52.94,0:03:55.69,ENG,,0,0,0,,The term was coined back in 1956.
Dialogue: 0,0:03:52.94,0:03:55.69,CHN,,0,0,0,,这个词早在 1956 年就被创造出来了
Dialogue: 0,0:03:55.69,0:03:58.76,ENG,,0,0,0,,In 1958, Cornell University psychologist,
Dialogue: 0,0:03:55.69,0:03:58.76,CHN,,0,0,0,,1958年  康奈尔大学的心理学家
Dialogue: 0,0:03:58.76,0:04:01.29,ENG,,0,0,0,,Frank Rosenblatt, built the perceptron,
Dialogue: 0,0:03:58.76,0:04:01.29,CHN,,0,0,0,,弗兰克·罗森布拉特  提出了感知机
Dialogue: 0,0:04:01.29,0:04:05.15,ENG,,0,0,0,,designed to mimic how neurons fire in our brains.
Dialogue: 0,0:04:01.29,0:04:05.15,CHN,,0,0,0,,来模仿人脑中的神经元发射信号的方式
Dialogue: 0,0:04:05.15,0:04:08.93,ENG,,0,0,0,,So here's a basic model of how neurons in our brains work.
Dialogue: 0,0:04:05.15,0:04:08.93,CHN,,0,0,0,,这是我们大脑中的神经元如何工作的基本模型
Dialogue: 0,0:04:08.93,0:04:12.16,ENG,,0,0,0,,An individual neuron can either fire or not,
Dialogue: 0,0:04:08.93,0:04:12.16,CHN,,0,0,0,,一个独立的神经元可以发射也可以不发射信号
Dialogue: 0,0:04:12.16,0:04:14.38,ENG,,0,0,0,,so its level of activation can be represented
Dialogue: 0,0:04:12.16,0:04:14.38,CHN,,0,0,0,,因此它的激活水平
Dialogue: 0,0:04:14.38,0:04:16.48,ENG,,0,0,0,,as a one or a zero.
Dialogue: 0,0:04:14.38,0:04:16.48,CHN,,0,0,0,,就可以表示为 0 或者 1
Dialogue: 0,0:04:16.48,0:04:18.37,ENG,,0,0,0,,The input to one neuron
Dialogue: 0,0:04:16.48,0:04:18.37,CHN,,0,0,0,,一个神经元的输入
Dialogue: 0,0:04:18.37,0:04:21.13,ENG,,0,0,0,,is the output from a bunch of other neurons,
Dialogue: 0,0:04:18.37,0:04:21.13,CHN,,0,0,0,,是其他一堆神经元的输出
Dialogue: 0,0:04:21.13,0:04:24.41,ENG,,0,0,0,,but the strength of these connections between neurons varies,
Dialogue: 0,0:04:21.13,0:04:24.41,CHN,,0,0,0,,但因为这些神经元之间的连接强度不同
Dialogue: 0,0:04:24.41,0:04:27.42,ENG,,0,0,0,,so each one can be given a different weight.
Dialogue: 0,0:04:24.41,0:04:27.42,CHN,,0,0,0,,所以每一个连接都有不同的权重
Dialogue: 0,0:04:27.42,0:04:29.41,ENG,,0,0,0,,Some connections are excitatory,
Dialogue: 0,0:04:27.42,0:04:29.41,CHN,,0,0,0,,有些连接起到刺激作用
Dialogue: 0,0:04:29.41,0:04:30.91,ENG,,0,0,0,,so they have positive weights,
Dialogue: 0,0:04:29.41,0:04:30.91,CHN,,0,0,0,,因此它们的权重是正的
Dialogue: 0,0:04:30.91,0:04:32.84,ENG,,0,0,0,,while others are inhibitory,
Dialogue: 0,0:04:30.91,0:04:32.84,CHN,,0,0,0,,另外一些连接起到抑制作用
Dialogue: 0,0:04:32.84,0:04:34.55,ENG,,0,0,0,,so they have negative weights.
Dialogue: 0,0:04:32.84,0:04:34.55,CHN,,0,0,0,,因此它们的权重是负的
Dialogue: 0,0:04:34.55,0:04:37.67,ENG,,0,0,0,,And the way to figure out whether a particular neuron fires,
Dialogue: 0,0:04:34.55,0:04:37.67,CHN,,0,0,0,,要判断某个神经元是否发射信号
Dialogue: 0,0:04:37.67,0:04:40.38,ENG,,0,0,0,,is to take the activation of each input neuron
Dialogue: 0,0:04:37.67,0:04:40.38,CHN,,0,0,0,,可以将它的每个输入神经元的激活水平
Dialogue: 0,0:04:40.38,0:04:42.60,ENG,,0,0,0,,and multiply by its weight,
Dialogue: 0,0:04:40.38,0:04:42.60,CHN,,0,0,0,,乘以它们的权重
Dialogue: 0,0:04:42.60,0:04:44.37,ENG,,0,0,0,,and then add these all together.
Dialogue: 0,0:04:42.60,0:04:44.37,CHN,,0,0,0,,再累加起来
Dialogue: 0,0:04:44.37,0:04:47.49,ENG,,0,0,0,,If their sum is greater than some number called the bias,
Dialogue: 0,0:04:44.37,0:04:47.49,CHN,,0,0,0,,如果它们的和大于某个数字（偏差）
Dialogue: 0,0:04:47.49,0:04:49.05,ENG,,0,0,0,,then the neuron fires,
Dialogue: 0,0:04:47.49,0:04:49.05,CHN,,0,0,0,,那么这个神经元就发射信号
Dialogue: 0,0:04:49.05,0:04:51.96,ENG,,0,0,0,,but if it's less than that, the neuron doesn't fire.
Dialogue: 0,0:04:49.05,0:04:51.96,CHN,,0,0,0,,如果小于偏差 那么就不发射
Dialogue: 0,0:04:53.46,0:04:57.19,ENG,,0,0,0,,As input, Rosenblatt's perceptron had 400 photocells
Dialogue: 0,0:04:53.46,0:04:57.19,CHN,,0,0,0,,罗森布拉特的感知机以 400 个感光器作为输入
Dialogue: 0,0:04:57.19,0:04:59.08,ENG,,0,0,0,,arranged in a square grid,
Dialogue: 0,0:04:57.19,0:04:59.08,CHN,,0,0,0,,感光器会排成正方形网格
Dialogue: 0,0:04:59.08,0:05:02.16,ENG,,0,0,0,,to capture a 20 by 20-pixel image.
Dialogue: 0,0:04:59.08,0:05:02.16,CHN,,0,0,0,,来捕捉 20×20 像素的图像
Dialogue: 0,0:05:02.16,0:05:04.67,ENG,,0,0,0,,You can think of each pixel as an input neuron,
Dialogue: 0,0:05:02.16,0:05:04.67,CHN,,0,0,0,,你可以认为每一个像素都是一个输入神经元
Dialogue: 0,0:05:04.67,0:05:07.78,ENG,,0,0,0,,with its activation being the brightness of the pixel.
Dialogue: 0,0:05:04.67,0:05:07.78,CHN,,0,0,0,,它的激活水平就是这个像素的亮度
Dialogue: 0,0:05:07.78,0:05:09.02,ENG,,0,0,0,,Although strictly speaking,
Dialogue: 0,0:05:07.78,0:05:09.02,CHN,,0,0,0,,严格来说
Dialogue: 0,0:05:09.02,0:05:11.91,ENG,,0,0,0,,the activation should be either zero or one,
Dialogue: 0,0:05:09.02,0:05:11.91,CHN,,0,0,0,,激活水平必须是 0 或 1
Dialogue: 0,0:05:11.91,0:05:15.96,ENG,,0,0,0,,we can let it take any value between zero and one.
Dialogue: 0,0:05:11.91,0:05:15.96,CHN,,0,0,0,,但我们可以让它取 0 到 1 之间的任何一个值
Dialogue: 0,0:05:15.96,0:05:20.13,ENG,,0,0,0,,All of these neurons are connected to a single output neuron,
Dialogue: 0,0:05:15.96,0:05:20.13,CHN,,0,0,0,,所有这些神经元都会连向一个输出神经元
Dialogue: 0,0:05:20.13,0:05:23.14,ENG,,0,0,0,,each via its own adjustable weight.
Dialogue: 0,0:05:20.13,0:05:23.14,CHN,,0,0,0,,每条连接都有一个可调节的权重
Dialogue: 0,0:05:23.14,0:05:25.37,ENG,,0,0,0,,So to see if the output neuron will fire,
Dialogue: 0,0:05:23.14,0:05:25.37,CHN,,0,0,0,,所以  要知道输出神经元是否发射信号
Dialogue: 0,0:05:25.37,0:05:28.84,ENG,,0,0,0,,you multiply the activation of each neuron by its weight,
Dialogue: 0,0:05:25.37,0:05:28.84,CHN,,0,0,0,,你可以将每个神经元的激活水平乘以它们的权重
Dialogue: 0,0:05:28.84,0:05:30.44,ENG,,0,0,0,,and add them together.
Dialogue: 0,0:05:28.84,0:05:30.44,CHN,,0,0,0,,再相加
Dialogue: 0,0:05:30.44,0:05:33.26,ENG,,0,0,0,,This is essentially a vector dot product.
Dialogue: 0,0:05:30.44,0:05:33.26,CHN,,0,0,0,,这个过程本质上是求向量点积
Dialogue: 0,0:05:33.26,0:05:36.60,ENG,,0,0,0,,If the answer is larger than the bias, the neuron fires,
Dialogue: 0,0:05:33.26,0:05:36.60,CHN,,0,0,0,,如果结果大于偏差  那么输出神经元发射信号
Dialogue: 0,0:05:36.60,0:05:38.89,ENG,,0,0,0,,and if not, it doesn't.
Dialogue: 0,0:05:36.60,0:05:38.89,CHN,,0,0,0,,反之不发射
Dialogue: 0,0:05:38.89,0:05:40.59,ENG,,0,0,0,,Now the goal of the perceptron
Dialogue: 0,0:05:38.89,0:05:40.59,CHN,,0,0,0,,现在  感知机的目标
Dialogue: 0,0:05:40.59,0:05:43.69,ENG,,0,0,0,,was to reliably distinguish between two images,
Dialogue: 0,0:05:40.59,0:05:43.69,CHN,,0,0,0,,是准确地分辨两张图片
Dialogue: 0,0:05:43.69,0:05:45.97,ENG,,0,0,0,,like a rectangle and a circle.
Dialogue: 0,0:05:43.69,0:05:45.97,CHN,,0,0,0,,比如分辨长方形和圆形
Dialogue: 0,0:05:45.97,0:05:46.80,ENG,,0,0,0,,For example,
Dialogue: 0,0:05:45.97,0:05:46.80,CHN,,0,0,0,,举个例子
Dialogue: 0,0:05:46.80,0:05:49.95,ENG,,0,0,0,,the output neuron could always fire when presented with a circle,
Dialogue: 0,0:05:46.80,0:05:49.95,CHN,,0,0,0,,当出现圆形时  输出神经元就会发射信号
Dialogue: 0,0:05:49.95,0:05:52.93,ENG,,0,0,0,,but never when presented with a rectangle.
Dialogue: 0,0:05:49.95,0:05:52.93,CHN,,0,0,0,,如果是长方形  那么它永远也不会发射信号
Dialogue: 0,0:05:52.93,0:05:55.89,ENG,,0,0,0,,To achieve this, the perception had to be trained,
Dialogue: 0,0:05:52.93,0:05:55.89,CHN,,0,0,0,,为了达到这个目的  我们需要训练感知机
Dialogue: 0,0:05:55.89,0:05:58.41,ENG,,0,0,0,,that is, shown a series of different circles
Dialogue: 0,0:05:55.89,0:05:58.41,CHN,,0,0,0,,也就是给它看一系列不同的圆形和长方形
Dialogue: 0,0:05:58.41,0:06:02.49,ENG,,0,0,0,,and rectangles, and have its weights adjusted accordingly.
Dialogue: 0,0:05:58.41,0:06:02.49,CHN,,0,0,0,,从而改变它的神经元连接权重
Dialogue: 0,0:06:02.49,0:06:05.35,ENG,,0,0,0,,We can visualize the weights as an image,
Dialogue: 0,0:06:02.49,0:06:05.35,CHN,,0,0,0,,因为图片的每个像素都有一个唯一的权重
Dialogue: 0,0:06:05.35,0:06:09.44,ENG,,0,0,0,,since there's a unique weight for each pixel of the image.
Dialogue: 0,0:06:05.35,0:06:09.44,CHN,,0,0,0,,所以我们可以把所有权重可视化为一张图像
Dialogue: 0,0:06:09.44,0:06:12.47,ENG,,0,0,0,,Initially, Rosenblatt set all the weights to zero.
Dialogue: 0,0:06:09.44,0:06:12.47,CHN,,0,0,0,,开始时  罗森布拉特把所有权重设置为 0
Dialogue: 0,0:06:12.47,0:06:14.53,ENG,,0,0,0,,If the perceptron's output is correct,
Dialogue: 0,0:06:12.47,0:06:14.53,CHN,,0,0,0,,如果感知机的输出正确
Dialogue: 0,0:06:14.53,0:06:16.82,ENG,,0,0,0,,for example, here it's shown a rectangle
Dialogue: 0,0:06:14.53,0:06:16.82,CHN,,0,0,0,,比如说  现在出现的是一个长方形
Dialogue: 0,0:06:16.82,0:06:19.00,ENG,,0,0,0,,and the output neuron doesn't fire,
Dialogue: 0,0:06:16.82,0:06:19.00,CHN,,0,0,0,,而且输出神经元没有发射信号
Dialogue: 0,0:06:19.00,0:06:21.26,ENG,,0,0,0,,no change is made to the weights.
Dialogue: 0,0:06:19.00,0:06:21.26,CHN,,0,0,0,,那么权重不会发生变化
Dialogue: 0,0:06:21.26,0:06:23.91,ENG,,0,0,0,,But if it's wrong, then the weights are adjusted.
Dialogue: 0,0:06:21.26,0:06:23.91,CHN,,0,0,0,,但如果感知机的输出是错的  就会调整权重
Dialogue: 0,0:06:23.91,0:06:25.92,ENG,,0,0,0,,The algorithm for updating the weights
Dialogue: 0,0:06:23.91,0:06:25.92,CHN,,0,0,0,,更新权重的算法非常简单
Dialogue: 0,0:06:25.92,0:06:27.64,ENG,,0,0,0,,is remarkably simple.
Dialogue: 0,0:06:25.92,0:06:27.64,CHN,,0,0,0,,更新权重的算法非常简单
Dialogue: 0,0:06:27.64,0:06:29.78,ENG,,0,0,0,,Here, the output neuron didn't fire
Dialogue: 0,0:06:27.64,0:06:29.78,CHN,,0,0,0,,现在给感知机展示的是一个圆形
Dialogue: 0,0:06:29.78,0:06:32.37,ENG,,0,0,0,,when it was supposed to because it was shown a circle.
Dialogue: 0,0:06:29.78,0:06:32.37,CHN,,0,0,0,,输出神经元应该发射信号  但它没有
Dialogue: 0,0:06:32.37,0:06:33.89,ENG,,0,0,0,,So to modify the weights,
Dialogue: 0,0:06:32.37,0:06:33.89,CHN,,0,0,0,,所以要修改权重
Dialogue: 0,0:06:33.89,0:06:38.21,ENG,,0,0,0,,you simply add the input activations to the weights.
Dialogue: 0,0:06:33.89,0:06:38.21,CHN,,0,0,0,,你可以简单地把输入激活水平加到权重上
Dialogue: 0,0:06:38.21,0:06:40.54,ENG,,0,0,0,,If the output neuron fires when it shouldn't,
Dialogue: 0,0:06:38.21,0:06:40.54,CHN,,0,0,0,,如果输出神经元在它不该发射信号时  发射了信号
Dialogue: 0,0:06:40.54,0:06:42.89,ENG,,0,0,0,,like here, when shown a rectangle,
Dialogue: 0,0:06:40.54,0:06:42.89,CHN,,0,0,0,,比如现在给它展示了一个长方形（但它发射了信号）
Dialogue: 0,0:06:42.89,0:06:46.74,ENG,,0,0,0,,well, then you subtract the input activations from the weights,
Dialogue: 0,0:06:42.89,0:06:46.74,CHN,,0,0,0,,你就要从权重上减去输入激活水平
Dialogue: 0,0:06:46.74,0:06:48.28,ENG,,0,0,0,,and you keep doing this
Dialogue: 0,0:06:46.74,0:06:48.28,CHN,,0,0,0,,一直重复进行这些操作
Dialogue: 0,0:06:48.28,0:06:52.63,ENG,,0,0,0,,until the perceptron correctly identifies all the training images.
Dialogue: 0,0:06:48.28,0:06:52.63,CHN,,0,0,0,,直到感知机能正确辨识  训练集里所有的图片
Dialogue: 0,0:06:52.63,0:06:55.52,ENG,,0,0,0,,It was shown that this algorithm will always converge,
Dialogue: 0,0:06:52.63,0:06:55.52,CHN,,0,0,0,,事实表明  这个算法一定会收敛
Dialogue: 0,0:06:55.52,0:06:58.18,ENG,,0,0,0,,so long as it's possible to map the two categories
Dialogue: 0,0:06:58.18,0:07:00.03,ENG,,0,0,0,,into distinct groups.
Dialogue: 0,0:06:55.52,0:07:00.03,CHN,,0,0,0,,前提是能把这两类图片映射到不同的组中
Dialogue: 0,0:07:02.24,0:07:04.13,ENG,,0,0,0,,The perceptron was capable
Dialogue: 0,0:07:02.24,0:07:04.13,CHN,,0,0,0,,感知机能够区分不同的形状
Dialogue: 0,0:07:04.13,0:07:06.03,ENG,,0,0,0,,of distinguishing between different shapes
Dialogue: 0,0:07:04.13,0:07:06.03,CHN,,0,0,0,,比如长方形和三角形
Dialogue: 0,0:07:06.03,0:07:09.22,ENG,,0,0,0,,like rectangles and triangles, or between different letters.
Dialogue: 0,0:07:06.03,0:07:09.22,CHN,,0,0,0,,它也可以区分不同的字母
Dialogue: 0,0:07:09.22,0:07:10.65,ENG,,0,0,0,,And according to Rosenblatt,
Dialogue: 0,0:07:09.22,0:07:10.65,CHN,,0,0,0,,据罗森布拉特所说
Dialogue: 0,0:07:10.65,0:07:14.02,ENG,,0,0,0,,it could even tell the difference between cats and dogs.
Dialogue: 0,0:07:10.65,0:07:14.02,CHN,,0,0,0,,它甚至还能区分猫和狗
Dialogue: 0,0:07:14.02,0:07:15.91,ENG,,0,0,0,,He said the machine was capable
Dialogue: 0,0:07:14.02,0:07:15.91,CHN,,0,0,0,,他说感知机能够提出
Dialogue: 0,0:07:15.91,0:07:18.94,ENG,,0,0,0,,of what amounts to original thought,
Dialogue: 0,0:07:15.91,0:07:18.94,CHN,,0,0,0,,相当于是原创的想法
Dialogue: 0,0:07:18.94,0:07:20.88,ENG,,0,0,0,,and the media lapped it up.
Dialogue: 0,0:07:18.94,0:07:20.88,CHN,,0,0,0,,媒体也大力赞扬感知机
Dialogue: 0,0:07:20.88,0:07:22.88,ENG,,0,0,0,,The "New York Times" called the perceptron
Dialogue: 0,0:07:20.88,0:07:22.88,CHN,,0,0,0,,纽约时报是这样评价感知机的：
Dialogue: 0,0:07:22.88,0:07:25.28,ENG,,0,0,0,,"the embryo of an electronic computer
Dialogue: 0,0:07:22.88,0:07:25.28,CHN,,0,0,0,,它是电子计算机的雏形
Dialogue: 0,0:07:25.28,0:07:28.35,ENG,,0,0,0,,that the Navy expects will be able to walk, talk,
Dialogue: 0,0:07:25.28,0:07:28.35,CHN,,0,0,0,,海军希望它未来能走路、能说话、
Dialogue: 0,0:07:28.35,0:07:30.89,ENG,,0,0,0,,see, write, reproduce itself,
Dialogue: 0,0:07:28.35,0:07:30.89,CHN,,0,0,0,,能看东西、能写字、能自我复制
Dialogue: 0,0:07:30.89,0:07:33.64,ENG,,0,0,0,,and be conscious of its existence."
Dialogue: 0,0:07:30.89,0:07:33.64,CHN,,0,0,0,,甚至能意识到自己的存在
Dialogue: 0,0:07:34.75,0:07:36.95,ENG,,0,0,0,,- After training on lots of examples,
Dialogue: 0,0:07:34.75,0:07:36.95,CHN,,0,0,0,,- 在经过大量训练后
Dialogue: 0,0:07:36.95,0:07:39.53,ENG,,0,0,0,,it's given new faces it has never seen,
Dialogue: 0,0:07:36.95,0:07:39.53,CHN,,0,0,0,,感知机会收到它从未见过的人脸图片
Dialogue: 0,0:07:39.53,0:07:43.48,ENG,,0,0,0,,and is able to successfully distinguish male from female.
Dialogue: 0,0:07:39.53,0:07:43.48,CHN,,0,0,0,,并且能够正确分辨这个人是男是女
Dialogue: 0,0:07:43.48,0:07:45.02,ENG,,0,0,0,,It has learned.
Dialogue: 0,0:07:43.48,0:07:45.02,CHN,,0,0,0,,它已经学会了
Dialogue: 0,0:07:45.02,0:07:48.73,ENG,,0,0,0,,- In reality, the perceptron was pretty limited in what it could do.
Dialogue: 0,0:07:45.02,0:07:48.73,CHN,,0,0,0,,实际上 感知机能做的事相当有限
Dialogue: 0,0:07:48.73,0:07:52.05,ENG,,0,0,0,,It could not, in fact, tell apart dogs from cats.
Dialogue: 0,0:07:48.73,0:07:52.05,CHN,,0,0,0,,其实它并不能区分猫和狗
Dialogue: 0,0:07:52.05,0:07:53.95,ENG,,0,0,0,,This and other critiques were raised
Dialogue: 0,0:07:52.05,0:07:53.95,CHN,,0,0,0,,1969年 麻省理工学院的巨头明斯基和帕珀特
Dialogue: 0,0:07:53.95,0:07:58.18,ENG,,0,0,0,,in a book by MIT giants, Minsky and Papert, in 1969.
Dialogue: 0,0:07:53.95,0:07:58.18,CHN,,0,0,0,,在书中指出了这一点  并给出了其他批评意见
Dialogue: 0,0:07:58.18,0:08:00.33,ENG,,0,0,0,,And that led to a bust period
Dialogue: 0,0:07:58.18,0:08:00.33,CHN,,0,0,0,,人工神经网络和整个人工智能
Dialogue: 0,0:08:00.33,0:08:03.53,ENG,,0,0,0,,for artificial neural networks and AI in general.
Dialogue: 0,0:08:00.33,0:08:03.53,CHN,,0,0,0,,因此陷入了萧条期
Dialogue: 0,0:08:03.53,0:08:06.72,ENG,,0,0,0,,It's known as the first AI winter.
Dialogue: 0,0:08:03.53,0:08:06.72,CHN,,0,0,0,,这被称为人工智能的第一个冬天
Dialogue: 0,0:08:06.72,0:08:09.37,ENG,,0,0,0,,Rosenblatt did not survive this winter.
Dialogue: 0,0:08:06.72,0:08:09.37,CHN,,0,0,0,,罗森布拉特没能熬过这个冬天
Dialogue: 0,0:08:09.37,0:08:12.20,ENG,,0,0,0,,He drowned while sailing in Chesapeake Bay
Dialogue: 0,0:08:12.20,0:08:14.30,ENG,,0,0,0,,on his 43rd birthday.
Dialogue: 0,0:08:09.37,0:08:14.30,CHN,,0,0,0,,他在 43 岁生日当天  在切萨皮克湾驾船时溺水身亡
Dialogue: 0,0:08:17.33,0:08:19.66,ENG,,0,0,0,,- The NAV Lab is a road-worthy truck,
Dialogue: 0,0:08:17.33,0:08:19.66,CHN,,0,0,0,,- NAV 实验室是一辆适航性货车
Dialogue: 0,0:08:19.66,0:08:22.29,ENG,,0,0,0,,modified so that researchers or computers
Dialogue: 0,0:08:19.66,0:08:22.29,CHN,,0,0,0,,它被改造后 能根据情况与需求
Dialogue: 0,0:08:22.29,0:08:25.36,ENG,,0,0,0,,can control the vehicle as occasion demands.
Dialogue: 0,0:08:22.29,0:08:25.36,CHN,,0,0,0,,由被研究员或电脑来操控
Dialogue: 0,0:08:25.36,0:08:28.09,ENG,,0,0,0,,- In the 1980s, there was an AI resurgence
Dialogue: 0,0:08:25.36,0:08:28.09,CHN,,0,0,0,,- 在20世纪80年代 卡耐基·梅隆大学的研究员在创造出其中一台
Dialogue: 0,0:08:28.09,0:08:29.64,ENG,,0,0,0,,when researchers at Carnegie Mellon
Dialogue: 0,0:08:28.09,0:08:29.64,CHN,,0,0,0,,当时世界上最早的自动驾驶汽车的时候
Dialogue: 0,0:08:29.64,0:08:32.47,ENG,,0,0,0,,created one of the first self-driving cars.
Dialogue: 0,0:08:29.64,0:08:32.47,CHN,,0,0,0,,曾经使人工智能一度再次兴起
Dialogue: 0,0:08:32.47,0:08:35.48,ENG,,0,0,0,,The vehicle was steered by an artificial neural network
Dialogue: 0,0:08:35.48,0:08:36.78,ENG,,0,0,0,,called ALVINN.
Dialogue: 0,0:08:32.47,0:08:36.78,CHN,,0,0,0,,这辆车子的转向 是由一个叫 ALVINN 的人工神经网络来控制的
Dialogue: 0,0:08:36.78,0:08:38.41,ENG,,0,0,0,,It was similar to the perceptron,
Dialogue: 0,0:08:36.78,0:08:38.41,CHN,,0,0,0,,它有点像感知器
Dialogue: 0,0:08:38.41,0:08:41.08,ENG,,0,0,0,,except it had a hidden layer of artificial neurons
Dialogue: 0,0:08:38.41,0:08:41.08,CHN,,0,0,0,,只不过它在输入与输出之间
Dialogue: 0,0:08:41.08,0:08:43.30,ENG,,0,0,0,,between the input and output.
Dialogue: 0,0:08:41.08,0:08:43.30,CHN,,0,0,0,,有一层隐藏起来的人工神经元
Dialogue: 0,0:08:43.30,0:08:45.14,ENG,,0,0,0,,As input, ALVINN received
Dialogue: 0,0:08:43.30,0:08:45.14,CHN,,0,0,0,,作为输入端  ALVINN 以 30*32 像素的图像
Dialogue: 0,0:08:45.14,0:08:48.40,ENG,,0,0,0,,30 by 32-pixel images of the road ahead.
Dialogue: 0,0:08:45.14,0:08:48.40,CHN,,0,0,0,,接收前面的路的信息
Dialogue: 0,0:08:48.40,0:08:51.60,ENG,,0,0,0,,Here, I'm showing them as 60 by 64 pixels.
Dialogue: 0,0:08:48.40,0:08:51.60,CHN,,0,0,0,,在这里我将以60*64像素的清晰度来呈现
Dialogue: 0,0:08:51.60,0:08:54.02,ENG,,0,0,0,,But each of these input neurons was connected
Dialogue: 0,0:08:51.60,0:08:54.02,CHN,,0,0,0,,但每一个输入的神经元都透过一个可调整的权重
Dialogue: 0,0:08:54.02,0:08:57.95,ENG,,0,0,0,,via an adjustable weight to a hidden layer of four neurons.
Dialogue: 0,0:08:54.02,0:08:57.95,CHN,,0,0,0,,被连接到一层隐藏起来的4个神经元
Dialogue: 0,0:08:57.95,0:09:01.60,ENG,,0,0,0,,These were each connected to 32 output neurons.
Dialogue: 0,0:08:57.95,0:09:01.60,CHN,,0,0,0,,而所有的这些隐藏层神经元 都连接到32个输出神经元
Dialogue: 0,0:09:01.60,0:09:04.18,ENG,,0,0,0,,So to go from one layer of the network to the next,
Dialogue: 0,0:09:01.60,0:09:04.18,CHN,,0,0,0,,因此要从这网络的一层去到它的下一层
Dialogue: 0,0:09:04.18,0:09:06.86,ENG,,0,0,0,,you perform a matrix multiplication:
Dialogue: 0,0:09:04.18,0:09:06.86,CHN,,0,0,0,,你必须要进行一次矩阵乘法
Dialogue: 0,0:09:06.86,0:09:10.12,ENG,,0,0,0,,the input activation times the weights.
Dialogue: 0,0:09:06.86,0:09:10.12,CHN,,0,0,0,,输入激励乘以权重
Dialogue: 0,0:09:10.12,0:09:12.58,ENG,,0,0,0,,The output neuron with the greatest activation
Dialogue: 0,0:09:10.12,0:09:12.58,CHN,,0,0,0,,输出神经元和最大的激励
Dialogue: 0,0:09:12.58,0:09:14.48,ENG,,0,0,0,,determines the steering angle.
Dialogue: 0,0:09:12.58,0:09:14.48,CHN,,0,0,0,,决定了方向盘的转动角度
Dialogue: 0,0:09:15.54,0:09:16.92,ENG,,0,0,0,,To train the neural net,
Dialogue: 0,0:09:15.54,0:09:16.92,CHN,,0,0,0,,要训练这个神经网络
Dialogue: 0,0:09:16.92,0:09:18.55,ENG,,0,0,0,,a human drove the vehicle,
Dialogue: 0,0:09:16.92,0:09:18.55,CHN,,0,0,0,,需要一个人去开这辆车
Dialogue: 0,0:09:18.55,0:09:22.61,ENG,,0,0,0,,providing the correct steering angle for a given input image.
Dialogue: 0,0:09:18.55,0:09:22.61,CHN,,0,0,0,,提供对于一个给定图像的正确的转动角
Dialogue: 0,0:09:22.61,0:09:24.11,ENG,,0,0,0,,All the weights in the neural network
Dialogue: 0,0:09:22.61,0:09:24.11,CHN,,0,0,0,,所有在这个神经网络里面的权重
Dialogue: 0,0:09:24.11,0:09:25.64,ENG,,0,0,0,,were adjusted through the training
Dialogue: 0,0:09:24.11,0:09:25.64,CHN,,0,0,0,,都在训练过程中被调整了
Dialogue: 0,0:09:25.64,0:09:29.06,ENG,,0,0,0,,so that ALVINN's output better matched that of the human driver.
Dialogue: 0,0:09:25.64,0:09:29.06,CHN,,0,0,0,,这样能使 ALVINN 的输出能与人类司机更匹配
Dialogue: 0,0:09:30.27,0:09:31.78,ENG,,0,0,0,,The method for adjusting the weights
Dialogue: 0,0:09:30.27,0:09:31.78,CHN,,0,0,0,,调整权重的方法
Dialogue: 0,0:09:31.78,0:09:33.39,ENG,,0,0,0,,is called backpropagation,
Dialogue: 0,0:09:31.78,0:09:33.39,CHN,,0,0,0,,被称为反向传播算法
Dialogue: 0,0:09:33.39,0:09:34.93,ENG,,0,0,0,,which I won't go into here,
Dialogue: 0,0:09:33.39,0:09:34.93,CHN,,0,0,0,,这我不会在这里深入探讨
Dialogue: 0,0:09:34.93,0:09:37.35,ENG,,0,0,0,,but Welch Labs has a great series on this,
Dialogue: 0,0:09:34.93,0:09:37.35,CHN,,0,0,0,,但 Welch Labs  频道有一系列的影片讲解这个算法
Dialogue: 0,0:09:37.35,0:09:39.25,ENG,,0,0,0,,which I'll link to in the description.
Dialogue: 0,0:09:37.35,0:09:39.25,CHN,,0,0,0,,
Dialogue: 0,0:09:40.09,0:09:41.90,ENG,,0,0,0,,Again, you can visualize the weights
Dialogue: 0,0:09:40.09,0:09:41.90,CHN,,0,0,0,,话说回来 你可以将这四个隐藏层神经元的权重
Dialogue: 0,0:09:41.90,0:09:44.50,ENG,,0,0,0,,for the four hidden neurons as images.
Dialogue: 0,0:09:41.90,0:09:44.50,CHN,,0,0,0,,视觉化成图像
Dialogue: 0,0:09:44.50,0:09:46.72,ENG,,0,0,0,,The weights are initially set to be random,
Dialogue: 0,0:09:44.50,0:09:46.72,CHN,,0,0,0,,在一开始 权重将被设置为随机数
Dialogue: 0,0:09:46.72,0:09:48.21,ENG,,0,0,0,,but as training progresses,
Dialogue: 0,0:09:46.72,0:09:48.21,CHN,,0,0,0,,但随着训练进行着
Dialogue: 0,0:09:48.21,0:09:51.76,ENG,,0,0,0,,the computer learns to pick up on certain patterns.
Dialogue: 0,0:09:48.21,0:09:51.76,CHN,,0,0,0,,计算机学会了如何去选一个特定的模式
Dialogue: 0,0:09:51.76,0:09:54.89,ENG,,0,0,0,,You can see the road markings emerge in the weights.
Dialogue: 0,0:09:51.76,0:09:54.89,CHN,,0,0,0,,你可以看到路的标线出现在那些权重里
Dialogue: 0,0:09:54.89,0:09:58.19,ENG,,0,0,0,,Simultaneously, the output steering angle coalesces
Dialogue: 0,0:09:54.89,0:09:58.19,CHN,,0,0,0,,而在同时 输出的转向角
Dialogue: 0,0:09:58.19,0:10:00.62,ENG,,0,0,0,,onto the human steering angle.
Dialogue: 0,0:09:58.19,0:10:00.62,CHN,,0,0,0,,也在朝人类转向的模式靠近
Dialogue: 0,0:10:00.62,0:10:03.08,ENG,,0,0,0,,The computer drove the vehicle at a top speed
Dialogue: 0,0:10:03.08,0:10:06.35,ENG,,0,0,0,,of around one or two kilometers per hour.
Dialogue: 0,0:10:00.62,0:10:06.35,CHN,,0,0,0,,计算机以每小时1-2公里的最高速度驾驶着这台汽车
Dialogue: 0,0:10:06.35,0:10:07.83,ENG,,0,0,0,,It was limited by the speed
Dialogue: 0,0:10:06.35,0:10:07.83,CHN,,0,0,0,,限制它最高速度的瓶颈
Dialogue: 0,0:10:07.83,0:10:10.76,ENG,,0,0,0,,at which the computer could perform matrix multiplication.
Dialogue: 0,0:10:07.83,0:10:10.76,CHN,,0,0,0,,是这台计算机进行矩阵乘法的速度极限
Dialogue: 0,0:10:12.25,0:10:13.70,ENG,,0,0,0,,Despite these advances,
Dialogue: 0,0:10:12.25,0:10:13.70,CHN,,0,0,0,,尽管取得了这些进展
Dialogue: 0,0:10:13.70,0:10:17.55,ENG,,0,0,0,,artificial neural networks still struggled with seemingly simple tasks,
Dialogue: 0,0:10:13.70,0:10:17.55,CHN,,0,0,0,,人工神经网络仍然很难完成看似简单的任务
Dialogue: 0,0:10:17.55,0:10:19.92,ENG,,0,0,0,,like telling apart cats and dogs.
Dialogue: 0,0:10:17.55,0:10:19.92,CHN,,0,0,0,,像分辨猫和狗之类的
Dialogue: 0,0:10:19.92,0:10:24.21,ENG,,0,0,0,,And no one knew whether hardware or software was the weak link.
Dialogue: 0,0:10:19.92,0:10:24.21,CHN,,0,0,0,,而且没有人知道这短板到底是来源于硬件还是软件
Dialogue: 0,0:10:24.21,0:10:26.56,ENG,,0,0,0,,I mean, did we have a good model of intelligence,
Dialogue: 0,0:10:24.21,0:10:26.56,CHN,,0,0,0,,换句话说 我们是否拥有一个很智能的模型
Dialogue: 0,0:10:26.56,0:10:28.59,ENG,,0,0,0,,we just needed more computer power?
Dialogue: 0,0:10:26.56,0:10:28.59,CHN,,0,0,0,,而我们所需要的 仅仅是更强的计算性能？
Dialogue: 0,0:10:28.59,0:10:30.47,ENG,,0,0,0,,Or, did we have the wrong idea
Dialogue: 0,0:10:28.59,0:10:30.47,CHN,,0,0,0,,抑或是 我们对如何构建智能系统
Dialogue: 0,0:10:30.47,0:10:33.55,ENG,,0,0,0,,about how to make intelligence systems altogether?
Dialogue: 0,0:10:30.47,0:10:33.55,CHN,,0,0,0,,拥有着完全错误的观念
Dialogue: 0,0:10:33.55,0:10:34.79,ENG,,0,0,0,,So artificial intelligence
Dialogue: 0,0:10:33.55,0:10:38.15,ENG,,0,0,0,,experienced another lull in the 1990s.
Dialogue: 0,0:10:33.55,0:10:38.15,CHN,,0,0,0,,所以人工智能 在20世纪90年代陷入了另一个低谷
Dialogue: 0,0:10:38.15,0:10:39.44,ENG,,0,0,0,,By the mid 2000s,
Dialogue: 0,0:10:38.15,0:10:39.44,CHN,,0,0,0,,直到 2000 年代中期
Dialogue: 0,0:10:39.44,0:10:43.25,ENG,,0,0,0,,most AI researchers were focused on improving algorithms.
Dialogue: 0,0:10:39.44,0:10:43.25,CHN,,0,0,0,,大多数人工智能研究者都专注于改进算法
Dialogue: 0,0:10:43.25,0:10:45.87,ENG,,0,0,0,,But one researcher, Fei-Fei Li,
Dialogue: 0,0:10:43.25,0:10:45.87,CHN,,0,0,0,,但有一位研究者——李飞飞
Dialogue: 0,0:10:45.87,0:10:48.26,ENG,,0,0,0,,thought maybe there was a different problem.
Dialogue: 0,0:10:45.87,0:10:48.26,CHN,,0,0,0,,认为或许该另觅他路
Dialogue: 0,0:10:48.26,0:10:50.35,ENG,,0,0,0,,Maybe these artificial neural networks
Dialogue: 0,0:10:48.26,0:10:50.35,CHN,,0,0,0,,可能这些人工神经网络
Dialogue: 0,0:10:50.35,0:10:52.54,ENG,,0,0,0,,just needed more data to train on.
Dialogue: 0,0:10:50.35,0:10:52.54,CHN,,0,0,0,,只是需要更多数据去训练
Dialogue: 0,0:10:52.54,0:10:56.17,ENG,,0,0,0,,So she planned to map out the entire world of objects.
Dialogue: 0,0:10:52.54,0:10:56.17,CHN,,0,0,0,,因此 她计划制作整个物体世界的地图
Dialogue: 0,0:10:56.17,0:10:59.22,ENG,,0,0,0,,From 2006 to 2009, she created ImageNet,
Dialogue: 0,0:10:56.17,0:10:59.22,CHN,,0,0,0,,2006 年到 2009 年期间 她创立了 ImageNet
Dialogue: 0,0:10:59.22,0:11:02.55,ENG,,0,0,0,,a database of 1.2 million human-labeled images,
Dialogue: 0,0:10:59.22,0:11:02.55,CHN,,0,0,0,,一个包含 120 万张人工标注图像的数据库
Dialogue: 0,0:11:02.55,0:11:03.38,ENG,,0,0,0,,which at the time,
Dialogue: 0,0:11:03.38,0:11:06.33,ENG,,0,0,0,,was the largest labeled image dataset ever constructed.
Dialogue: 0,0:11:02.55,0:11:06.33,CHN,,0,0,0,,在当时 是有史以来最大的标记图像数据集
Dialogue: 0,0:11:06.33,0:11:08.25,ENG,,0,0,0,,And from 2010 to 2017,
Dialogue: 0,0:11:06.33,0:11:08.25,CHN,,0,0,0,,从 2010 年到 2017 年
Dialogue: 0,0:11:08.25,0:11:10.40,ENG,,0,0,0,,ImageNet ran an annual contest:
Dialogue: 0,0:11:08.25,0:11:10.40,CHN,,0,0,0,,ImageNet 举办每年一度的竞赛
Dialogue: 0,0:11:10.40,0:11:13.74,ENG,,0,0,0,,the ImageNet Large Scale Visual Recognition Challenge,
Dialogue: 0,0:11:10.40,0:11:13.74,CHN,,0,0,0,,ImageNet 大规模视觉识别挑战赛（ILSVRC）
Dialogue: 0,0:11:13.74,0:11:15.24,ENG,,0,0,0,,where software programs competed
Dialogue: 0,0:11:13.74,0:11:15.24,CHN,,0,0,0,,让软件程序互相竞争
Dialogue: 0,0:11:15.24,0:11:17.97,ENG,,0,0,0,,to correctly detect and classify images.
Dialogue: 0,0:11:15.24,0:11:17.97,CHN,,0,0,0,,试图正确检测和分类图像
Dialogue: 0,0:11:17.97,0:11:21.11,ENG,,0,0,0,,Images were classified into 1,000 different categories,
Dialogue: 0,0:11:17.97,0:11:21.11,CHN,,0,0,0,,测试用图像分为 1000 个不同类别
Dialogue: 0,0:11:21.11,0:11:23.52,ENG,,0,0,0,,including 90 different dog breeds.
Dialogue: 0,0:11:21.11,0:11:23.52,CHN,,0,0,0,,其中包括 90 个不同犬种
Dialogue: 0,0:11:23.52,0:11:25.46,ENG,,0,0,0,,A neural network competing in this competition
Dialogue: 0,0:11:23.52,0:11:25.46,CHN,,0,0,0,,参与这场比拼的神经网络
Dialogue: 0,0:11:25.46,0:11:28.22,ENG,,0,0,0,,would have an output layer of 1,000 neurons,
Dialogue: 0,0:11:25.46,0:11:28.22,CHN,,0,0,0,,输出层包含 1000 个神经元
Dialogue: 0,0:11:28.22,0:11:30.46,ENG,,0,0,0,,each corresponding to a category of object
Dialogue: 0,0:11:30.46,0:11:32.23,ENG,,0,0,0,,that could appear in the image.
Dialogue: 0,0:11:28.22,0:11:32.23,CHN,,0,0,0,,分别对应图像中出现的 1000 个物体大类
Dialogue: 0,0:11:32.23,0:11:34.50,ENG,,0,0,0,,If the image contains, say, a German shepherd,
Dialogue: 0,0:11:32.23,0:11:34.50,CHN,,0,0,0,,如果图像中包含 比方说 一只德国牧羊犬
Dialogue: 0,0:11:34.50,0:11:37.43,ENG,,0,0,0,,then the output neuron corresponding to German shepherd
Dialogue: 0,0:11:34.50,0:11:37.43,CHN,,0,0,0,,那么对应德国牧羊犬的那个输出神经元
Dialogue: 0,0:11:37.43,0:11:39.77,ENG,,0,0,0,,should have the highest activation.
Dialogue: 0,0:11:37.43,0:11:39.77,CHN,,0,0,0,,就应该激活最高的输出
Dialogue: 0,0:11:39.77,0:11:43.12,ENG,,0,0,0,,Unsurprisingly, it turned out to be a tough challenge.
Dialogue: 0,0:11:39.77,0:11:43.12,CHN,,0,0,0,,不出所料 事实证明这是一项艰巨的挑战
Dialogue: 0,0:11:43.12,0:11:45.13,ENG,,0,0,0,,One way to judge the performance of an AI
Dialogue: 0,0:11:43.12,0:11:45.13,CHN,,0,0,0,,评判 AI 性能的一种方式
Dialogue: 0,0:11:45.13,0:11:48.36,ENG,,0,0,0,,is to see how often the five highest neuron activations
Dialogue: 0,0:11:45.13,0:11:48.36,CHN,,0,0,0,,是看激活输出最高的 5 个神经元
Dialogue: 0,0:11:48.36,0:11:50.92,ENG,,0,0,0,,do not include the correct category.
Dialogue: 0,0:11:48.36,0:11:50.92,CHN,,0,0,0,,均不是正确类别的频率
Dialogue: 0,0:11:50.92,0:11:53.84,ENG,,0,0,0,,This is the so-called top-5 error rate.
Dialogue: 0,0:11:50.92,0:11:53.84,CHN,,0,0,0,,这就是所谓的 Top-5 错误率
Dialogue: 0,0:11:53.84,0:11:58.82,ENG,,0,0,0,,In 2010, the best performer had a top-5 error rate of 28.2%,
Dialogue: 0,0:11:53.84,0:11:58.82,CHN,,0,0,0,,2010 年 表现最佳的程序达到了 28.2%
Dialogue: 0,0:11:58.82,0:12:01.08,ENG,,0,0,0,,meaning that nearly 1/3 of the time,
Dialogue: 0,0:11:58.82,0:12:01.08,CHN,,0,0,0,,这意味着在近三分之一的测试中
Dialogue: 0,0:12:01.08,0:12:04.57,ENG,,0,0,0,,the correct answer was not among its top five guesses.
Dialogue: 0,0:12:01.08,0:12:04.57,CHN,,0,0,0,,正确答案都不在其前 5 个猜测之列
Dialogue: 0,0:12:04.57,0:12:09.27,ENG,,0,0,0,,In 2011, the error rate of the best performer was 25.8%,
Dialogue: 0,0:12:04.57,0:12:09.27,CHN,,0,0,0,,2011 年 这个数字降低到 25.8%
Dialogue: 0,0:12:09.27,0:12:11.36,ENG,,0,0,0,,a substantial improvement.
Dialogue: 0,0:12:09.27,0:12:11.36,CHN,,0,0,0,,这是很大的进步
Dialogue: 0,0:12:11.36,0:12:12.37,ENG,,0,0,0,,But the next year,
Dialogue: 0,0:12:11.36,0:12:12.37,CHN,,0,0,0,,但是接下来的一年
Dialogue: 0,0:12:12.37,0:12:13.62,ENG,,0,0,0,,an artificial neural network
Dialogue: 0,0:12:13.62,0:12:16.22,ENG,,0,0,0,,from the University of Toronto, called AlexNet,
Dialogue: 0,0:12:12.37,0:12:16.22,CHN,,0,0,0,,来自多伦多大学的人工神经网络 AlexNet
Dialogue: 0,0:12:16.22,0:12:17.87,ENG,,0,0,0,,blew away the competition
Dialogue: 0,0:12:17.87,0:12:22.41,ENG,,0,0,0,,with a top-5 error rate of just 16.4%.
Dialogue: 0,0:12:16.22,0:12:22.41,CHN,,0,0,0,,以低至 16.4% 的 Top-5 错误率碾压对手
Dialogue: 0,0:12:22.41,0:12:25.84,ENG,,0,0,0,,What set AlexNet apart was its size and depth.
Dialogue: 0,0:12:22.41,0:12:25.84,CHN,,0,0,0,,AlexNet 的规模和深度使之与众不同
Dialogue: 0,0:12:25.84,0:12:27.72,ENG,,0,0,0,,The network consisted of eight layers,
Dialogue: 0,0:12:25.84,0:12:27.72,CHN,,0,0,0,,该网络共有个 8 神经层
Dialogue: 0,0:12:27.72,0:12:30.70,ENG,,0,0,0,,and in total, 500,000 neurons.
Dialogue: 0,0:12:27.72,0:12:30.70,CHN,,0,0,0,,总共包含 50 万神经元
Dialogue: 0,0:12:30.70,0:12:31.53,ENG,,0,0,0,,To train AlexNet,
Dialogue: 0,0:12:30.70,0:12:33.53,CHN,,0,0,0,,要训练 AlexNet 得通过训练集
Dialogue: 0,0:12:31.53,0:12:35.58,ENG,,0,0,0,,60 million weights and biases had to be carefully adjusted
Dialogue: 0,0:12:35.58,0:12:37.50,ENG,,0,0,0,,using the training database.
Dialogue: 0,0:12:33.58,0:12:37.50,CHN,,0,0,0,,小心调整 6000 万个权重与偏置
Dialogue: 0,0:12:37.50,0:12:40.04,ENG,,0,0,0,,Because of all the big matrix multiplications,
Dialogue: 0,0:12:37.50,0:12:40.04,CHN,,0,0,0,,由于这些巨大的矩阵乘法运算
Dialogue: 0,0:12:40.04,0:12:43.57,ENG,,0,0,0,,processing a single image required 700 million
Dialogue: 0,0:12:43.57,0:12:45.33,ENG,,0,0,0,,individual math operations.
Dialogue: 0,0:12:40.04,0:12:45.33,CHN,,0,0,0,,处理一张图片需要 7 亿次简单数学运算
Dialogue: 0,0:12:45.33,0:12:48.28,ENG,,0,0,0,,So training was computationally intensive.
Dialogue: 0,0:12:45.33,0:12:48.28,CHN,,0,0,0,,因此 训练它 是计算密集的
Dialogue: 0,0:12:48.28,0:12:51.33,ENG,,0,0,0,,The team managed it by pioneering the use of GPUs,
Dialogue: 0,0:12:48.28,0:12:51.33,CHN,,0,0,0,,它的团队开创性地使用 GPU 完成计算
Dialogue: 0,0:12:51.33,0:12:52.91,ENG,,0,0,0,,graphical processing units,
Dialogue: 0,0:12:51.33,0:12:52.91,CHN,,0,0,0,,GPU 即图形处理单元
Dialogue: 0,0:12:52.91,0:12:56.33,ENG,,0,0,0,,which are traditionally used for driving displays, screens.
Dialogue: 0,0:12:52.91,0:12:56.33,CHN,,0,0,0,,通常用于驱动显示器、屏幕等
Dialogue: 0,0:12:56.33,0:13:00.11,ENG,,0,0,0,,So they're specialized for fast parallel computations.
Dialogue: 0,0:12:56.33,0:13:00.11,CHN,,0,0,0,,因此适合快速并行计算
Dialogue: 0,0:13:00.11,0:13:02.52,ENG,,0,0,0,,The AlexNet paper describing their research
Dialogue: 0,0:13:02.52,0:13:04.26,ENG,,0,0,0,,is a blockbuster.
Dialogue: 0,0:13:00.11,0:13:04.26,CHN,,0,0,0,,描述他们研究的 AlexNet 论文引起轰动
Dialogue: 0,0:13:04.26,0:13:07.68,ENG,,0,0,0,,It's now been cited over 100,000 times,
Dialogue: 0,0:13:04.26,0:13:07.68,CHN,,0,0,0,,现在已经被引用十万余次
Dialogue: 0,0:13:07.68,0:13:10.39,ENG,,0,0,0,,and it identifies the scale of the neural network
Dialogue: 0,0:13:10.39,0:13:12.98,ENG,,0,0,0,,as key to its success.
Dialogue: 0,0:13:07.68,0:13:12.98,CHN,,0,0,0,,论文认为 神经网络的规模是其成功的关键
Dialogue: 0,0:13:12.98,0:13:16.16,ENG,,0,0,0,,It takes a lot of computation to train and run the network,
Dialogue: 0,0:13:12.98,0:13:16.16,CHN,,0,0,0,,训练和运行这一网络需要大量的计算
Dialogue: 0,0:13:16.16,0:13:19.29,ENG,,0,0,0,,but the improvement in performance is worth it.
Dialogue: 0,0:13:16.16,0:13:19.29,CHN,,0,0,0,,但相较于性能的提升 是值得的
Dialogue: 0,0:13:19.29,0:13:20.74,ENG,,0,0,0,,With others following their lead,
Dialogue: 0,0:13:19.29,0:13:20.74,CHN,,0,0,0,,在后来者的努力下
Dialogue: 0,0:13:20.74,0:13:23.39,ENG,,0,0,0,,the top-5 error rate on the ImageNet competition
Dialogue: 0,0:13:20.74,0:13:23.39,CHN,,0,0,0,,ImageNet 挑战赛的 Top-5 错误率
Dialogue: 0,0:13:23.39,0:13:25.02,ENG,,0,0,0,,plummeted in the years that followed,
Dialogue: 0,0:13:23.39,0:13:25.02,CHN,,0,0,0,,在随后的几年直线下降
Dialogue: 0,0:13:25.02,0:13:28.21,ENG,,0,0,0,,down to 3.6% in 2015.
Dialogue: 0,0:13:25.02,0:13:28.21,CHN,,0,0,0,,到 2015 年已经降至 3.6%
Dialogue: 0,0:13:28.21,0:13:31.16,ENG,,0,0,0,,That is better than human performance.
Dialogue: 0,0:13:28.21,0:13:31.16,CHN,,0,0,0,,这已经超越了人类表现
Dialogue: 0,0:13:31.16,0:13:32.73,ENG,,0,0,0,,The neural network that achieved this
Dialogue: 0,0:13:32.73,0:13:35.03,ENG,,0,0,0,,had 100 layers of neurons.
Dialogue: 0,0:13:31.16,0:13:35.03,CHN,,0,0,0,,实现这一目标的神经网络包含 100 个神经层
Dialogue: 0,0:13:35.03,0:13:36.49,ENG,,0,0,0,,So the future is clear:
Dialogue: 0,0:13:35.03,0:13:36.49,CHN,,0,0,0,,所以未来是明确的
Dialogue: 0,0:13:36.49,0:13:38.39,ENG,,0,0,0,,We will see ever increasing demand
Dialogue: 0,0:13:38.39,0:13:40.82,ENG,,0,0,0,,for ever larger neural networks.
Dialogue: 0,0:13:36.49,0:13:40.82,CHN,,0,0,0,,我们将看到对更大神经网络的需求不断增长
Dialogue: 0,0:13:40.82,0:13:43.04,ENG,,0,0,0,,And this is a problem for several reasons:
Dialogue: 0,0:13:40.82,0:13:43.04,CHN,,0,0,0,,从数个方面来看 这都是一个问题
Dialogue: 0,0:13:43.04,0:13:45.15,ENG,,0,0,0,,One is energy consumption.
Dialogue: 0,0:13:43.04,0:13:45.15,CHN,,0,0,0,,其一是能源消耗
Dialogue: 0,0:13:45.15,0:13:47.06,ENG,,0,0,0,,Training a neural network requires an amount
Dialogue: 0,0:13:45.15,0:13:47.06,CHN,,0,0,0,,训练一个神经网络所需的电量
Dialogue: 0,0:13:47.06,0:13:49.46,ENG,,0,0,0,,of electricity similar to the yearly consumption
Dialogue: 0,0:13:49.46,0:13:50.99,ENG,,0,0,0,,of three households.
Dialogue: 0,0:13:47.06,0:13:50.99,CHN,,0,0,0,,相当于三个家庭的一年的用电量
Dialogue: 0,0:13:50.99,0:13:52.59,ENG,,0,0,0,,Another issue is the so-called
Dialogue: 0,0:13:50.99,0:13:52.59,CHN,,0,0,0,,另一个问题被称为
Dialogue: 0,0:13:52.59,0:13:54.10,CHN,,0,0,0,,冯·诺伊曼瓶颈
Dialogue: 0,0:13:54.10,0:13:55.87,ENG,,0,0,0,,Virtually every modern digital computer
Dialogue: 0,0:13:55.87,0:13:57.20,ENG,,0,0,0,,stores data in memory,
Dialogue: 0,0:13:54.10,0:13:57.20,CHN,,0,0,0,,几乎所有现代数字计算机都将数据存入内存
Dialogue: 0,0:13:57.20,0:14:00.36,ENG,,0,0,0,,and then accesses it as needed over a bus.
Dialogue: 0,0:13:57.20,0:14:00.36,CHN,,0,0,0,,然后按照需要 通过一条总线访问它
Dialogue: 0,0:14:00.36,0:14:02.44,ENG,,0,0,0,,When performing the huge matrix multiplications
Dialogue: 0,0:14:02.44,0:14:04.23,ENG,,0,0,0,,required by deep neural networks,
Dialogue: 0,0:14:00.36,0:14:04.23,CHN,,0,0,0,,当执行深度神经网络所需的大量矩阵乘法时
Dialogue: 0,0:14:04.23,0:14:05.36,ENG,,0,0,0,,most of the time and energy
Dialogue: 0,0:14:05.36,0:14:07.66,ENG,,0,0,0,,goes into fetching those weight values
Dialogue: 0,0:14:04.23,0:14:07.66,CHN,,0,0,0,,大部分时间和能源都用于获取这些数值
Dialogue: 0,0:14:07.66,0:14:10.49,ENG,,0,0,0,,rather than actually doing the computation.
Dialogue: 0,0:14:07.66,0:14:10.49,CHN,,0,0,0,,而不是实际的数学运算
Dialogue: 0,0:14:10.49,0:14:13.25,ENG,,0,0,0,,And finally, there are the limitations of Moore's Law.
Dialogue: 0,0:14:10.49,0:14:13.25,CHN,,0,0,0,,最后 还有摩尔定律的局限性
Dialogue: 0,0:14:13.25,0:14:15.41,ENG,,0,0,0,,For decades, the number of transistors on a chip
Dialogue: 0,0:14:15.41,0:14:18.19,ENG,,0,0,0,,has been doubling approximately every two years,
Dialogue: 0,0:14:13.25,0:14:18.19,CHN,,0,0,0,,几十年来 芯片上的晶体管数量约两年翻一番
Dialogue: 0,0:14:18.19,0:14:20.13,ENG,,0,0,0,,but now the size of a transistor
Dialogue: 0,0:14:20.13,0:14:21.91,ENG,,0,0,0,,is approaching the size of an atom.
Dialogue: 0,0:14:18.19,0:14:21.91,CHN,,0,0,0,,但现在晶体管的尺寸已经接近原子大小
Dialogue: 0,0:14:21.91,0:14:24.69,ENG,,0,0,0,,So there are some fundamental physical challenges
Dialogue: 0,0:14:21.91,0:14:24.69,CHN,,0,0,0,,因此进一步小型化
Dialogue: 0,0:14:24.69,0:14:26.88,ENG,,0,0,0,,to further miniaturization.
Dialogue: 0,0:14:24.69,0:14:26.88,CHN,,0,0,0,,在物理原理上面临挑战
Dialogue: 0,0:14:26.88,0:14:30.27,ENG,,0,0,0,,So this is the perfect storm for analog computers.
Dialogue: 0,0:14:26.88,0:14:30.27,CHN,,0,0,0,,因此 模拟计算机正逢其时
Dialogue: 0,0:14:30.27,0:14:32.64,ENG,,0,0,0,,Digital computers are reaching their limits.
Dialogue: 0,0:14:30.27,0:14:32.64,CHN,,0,0,0,,数字计算机正在接近其极限
Dialogue: 0,0:14:32.64,0:14:35.88,ENG,,0,0,0,,Meanwhile, neural networks are exploding in popularity,
Dialogue: 0,0:14:32.64,0:14:35.88,CHN,,0,0,0,,与此同时 神经网络正在爆炸式发展
Dialogue: 0,0:14:35.88,0:14:39.55,ENG,,0,0,0,,and a lot of what they do boils down to a single task:
Dialogue: 0,0:14:35.88,0:14:39.55,CHN,,0,0,0,,其所需的大部分操作 归结为一个单一任务
Dialogue: 0,0:14:39.55,0:14:41.29,ENG,,0,0,0,,matrix multiplication.
Dialogue: 0,0:14:39.55,0:14:41.29,CHN,,0,0,0,,矩阵乘法
Dialogue: 0,0:14:41.29,0:14:42.78,ENG,,0,0,0,,Best of all, neural networks
Dialogue: 0,0:14:42.78,0:14:45.35,ENG,,0,0,0,,don't need the precision of digital computers.
Dialogue: 0,0:14:41.29,0:14:45.35,CHN,,0,0,0,,最重要的是 神经网络不需要数字计算机提供的精度
Dialogue: 0,0:14:45.35,0:14:48.99,ENG,,0,0,0,,Whether the neural net is 96% or 98% confident
Dialogue: 0,0:14:45.35,0:14:48.99,CHN,,0,0,0,,神经网络若认为图中包含一只鸡
Dialogue: 0,0:14:48.99,0:14:50.41,ENG,,0,0,0,,the image contains a chicken,
Dialogue: 0,0:14:48.99,0:14:50.41,CHN,,0,0,0,,不论置信度是 96% 或 98%
Dialogue: 0,0:14:50.41,0:14:52.81,ENG,,0,0,0,,it doesn't really matter, it's still a chicken.
Dialogue: 0,0:14:50.41,0:14:52.81,CHN,,0,0,0,,归根结底那仍是一只鸡
Dialogue: 0,0:14:52.81,0:14:54.87,ENG,,0,0,0,,So slight variability in components
Dialogue: 0,0:14:54.87,0:14:57.37,ENG,,0,0,0,,or conditions can be tolerated.
Dialogue: 0,0:14:52.81,0:14:57.37,CHN,,0,0,0,,因此 零件和环境的轻微变化是可以容忍的
Dialogue: 0,0:14:58.81,0:15:01.37,ENG,,0,0,0,,I went to an analog computing startup in Texas,
Dialogue: 0,0:14:58.81,0:15:01.37,CHN,,0,0,0,,我造访了德州的一家初创公司
Dialogue: 0,0:15:01.37,0:15:03.37,ENG,,0,0,0,,called Mythic AI.
Dialogue: 0,0:15:01.37,0:15:03.37,CHN,,0,0,0,, Mythic AI （迷思智能？）
Dialogue: 0,0:15:03.37,0:15:06.82,ENG,,0,0,0,,Here, they're creating analog chips to run neural networks.
Dialogue: 0,0:15:03.37,0:15:06.82,CHN,,0,0,0,,他们在这里创制模拟芯片以运行神经网络
Dialogue: 0,0:15:06.82,0:15:09.92,ENG,,0,0,0,,And they demonstrated several AI algorithms for me.
Dialogue: 0,0:15:06.82,0:15:09.92,CHN,,0,0,0,,并且为我展示了几项人工智能算法
Dialogue: 0,0:15:10.98,0:15:11.81,ENG,,0,0,0,,- Oh, there you go.
Dialogue: 0,0:15:10.98,0:15:11.81,CHN,,0,0,0,,哦 快看
Dialogue: 0,0:15:11.81,0:15:13.29,ENG,,0,0,0,,See, it's getting you.
Dialogue: 0,0:15:11.81,0:15:13.29,CHN,,0,0,0,,它认出你了
Dialogue: 0,0:15:13.29,0:15:14.77,ENG,,0,0,0,,Yeah. - That's fascinating.
Dialogue: 0,0:15:13.29,0:15:14.77,CHN,,0,0,0,,是的 - 这超棒的
Dialogue: 0,0:15:14.77,0:15:17.63,ENG,,0,0,0,,- The biggest use case is augmented and virtual reality.
Dialogue: 0,0:15:14.77,0:15:17.63,CHN,,0,0,0,,它主要应用在增强现实和虚拟现实中
Dialogue: 0,0:15:17.63,0:15:19.15,ENG,,0,0,0,,If your friend is in a different,
Dialogue: 0,0:15:17.63,0:15:19.15,CHN,,0,0,0,,如果你的朋友们天各一方
Dialogue: 0,0:15:19.15,0:15:20.71,ENG,,0,0,0,,they're at their house and you're at your house,
Dialogue: 0,0:15:19.15,0:15:20.71,CHN,,0,0,0,,他们和你分别在各自家中
Dialogue: 0,0:15:20.71,0:15:24.12,ENG,,0,0,0,,you can actually render each other in the virtual world.
Dialogue: 0,0:15:20.71,0:15:24.12,CHN,,0,0,0,,你们要能在虚拟世界中渲染出对方的样子
Dialogue: 0,0:15:24.12,0:15:27.28,ENG,,0,0,0,,So it needs to really quickly capture your pose,
Dialogue: 0,0:15:24.12,0:15:27.28,CHN,,0,0,0,,这就需要高效的动作捕捉
Dialogue: 0,0:15:27.28,0:15:29.35,ENG,,0,0,0,,and then render it in the VR world.
Dialogue: 0,0:15:27.28,0:15:29.35,CHN,,0,0,0,,以及在 VR 世界中的渲染
Dialogue: 0,0:15:29.35,0:15:31.33,ENG,,0,0,0,,- So, hang on, is this for the metaverse thing?
Dialogue: 0,0:15:29.35,0:15:31.33,CHN,,0,0,0,,所以 等等 这是给元宇宙准备的吗
Dialogue: 0,0:15:31.33,0:15:35.64,ENG,,0,0,0,,- Yeah, this is a very metaverse application.
Dialogue: 0,0:15:31.33,0:15:35.64,CHN,,0,0,0,,是的 这是个很有元宇宙范的应用
Dialogue: 0,0:15:35.64,0:15:38.63,ENG,,0,0,0,,This is depth estimation from just a single webcam.
Dialogue: 0,0:15:35.64,0:15:38.63,CHN,,0,0,0,,这是基于单摄像头的景深估计
Dialogue: 0,0:15:38.63,0:15:39.95,ENG,,0,0,0,,It's just taking this scene,
Dialogue: 0,0:15:38.63,0:15:39.95,CHN,,0,0,0,,它只是拍摄了这幅画面
Dialogue: 0,0:15:39.95,0:15:41.36,ENG,,0,0,0,,and then it's doing a heat map.
Dialogue: 0,0:15:39.95,0:15:41.36,CHN,,0,0,0,,然后把它转换成热力图
Dialogue: 0,0:15:41.36,0:15:43.75,ENG,,0,0,0,,So if it's bright, it means it's close.
Dialogue: 0,0:15:41.36,0:15:43.75,CHN,,0,0,0,,图上亮的地方意味着更靠近一些
Dialogue: 0,0:15:43.75,0:15:45.98,ENG,,0,0,0,,And if it's far away, it makes it like black.
Dialogue: 0,0:15:43.75,0:15:45.98,CHN,,0,0,0,,如果离远一些 颜色就会变深
Dialogue: 0,0:15:45.98,0:15:48.85,ENG,,0,0,0,,- Now all these algorithms can be run on digital computers,
Dialogue: 0,0:15:45.98,0:15:48.85,CHN,,0,0,0,,- 目前以上所有算法都能在数字计算机上运行
Dialogue: 0,0:15:48.85,0:15:51.01,ENG,,0,0,0,,but here, the matrix multiplication
Dialogue: 0,0:15:48.85,0:15:51.01,CHN,,0,0,0,,但是在这里 矩阵乘法
Dialogue: 0,0:15:51.01,0:15:54.42,ENG,,0,0,0,,is actually taking place in the analog domain.
Dialogue: 0,0:15:51.01,0:15:54.42,CHN,,0,0,0,,其实是在模拟域进行的
Dialogue: 0,0:15:54.42,0:15:55.80,ENG,,0,0,0,,To make this possible,
Dialogue: 0,0:15:54.42,0:15:55.80,CHN,,0,0,0,,为了实现这一点
Dialogue: 0,0:15:55.80,0:15:59.49,ENG,,0,0,0,,Mythic has repurposed digital flash storage cells.
Dialogue: 0,0:15:55.80,0:15:59.49,CHN,,0,0,0,,Mythic 调整了数字闪存单元的用途
Dialogue: 0,0:15:59.49,0:16:01.21,ENG,,0,0,0,,Normally these are used as memory
Dialogue: 0,0:15:59.49,0:16:01.21,CHN,,0,0,0,,它们一般被用作内存
Dialogue: 0,0:16:01.21,0:16:03.63,ENG,,0,0,0,,to store either a one or a zero.
Dialogue: 0,0:16:01.21,0:16:03.63,CHN,,0,0,0,,每一位上记录的要么是1 要么是0
Dialogue: 0,0:16:03.63,0:16:07.46,ENG,,0,0,0,,If you apply a large positive voltage to the control gate,
Dialogue: 0,0:16:03.63,0:16:07.46,CHN,,0,0,0,,如果你在控制门上施加较大的电压
Dialogue: 0,0:16:07.46,0:16:10.21,ENG,,0,0,0,,electrons tunnel up through an insulating barrier
Dialogue: 0,0:16:07.46,0:16:10.21,CHN,,0,0,0,,电子将隧穿通过绝缘层
Dialogue: 0,0:16:10.21,0:16:12.44,ENG,,0,0,0,,and become trapped on the floating gate.
Dialogue: 0,0:16:10.21,0:16:12.44,CHN,,0,0,0,,并被限制在悬浮门中
Dialogue: 0,0:16:12.44,0:16:13.57,ENG,,0,0,0,,Remove the voltage,
Dialogue: 0,0:16:12.44,0:16:13.57,CHN,,0,0,0,,即使撤去电压
Dialogue: 0,0:16:13.57,0:16:16.22,ENG,,0,0,0,,and the electrons can remain on the floating gate for decades,
Dialogue: 0,0:16:13.57,0:16:16.22,CHN,,0,0,0,,这些电子能在悬浮门中停留数十年
Dialogue: 0,0:16:16.22,0:16:18.60,ENG,,0,0,0,,preventing the cell from conducting current.
Dialogue: 0,0:16:16.22,0:16:18.60,CHN,,0,0,0,,阻止闪存单元导通
Dialogue: 0,0:16:18.60,0:16:21.32,ENG,,0,0,0,,And that's how you can store either a one or a zero.
Dialogue: 0,0:16:18.60,0:16:21.32,CHN,,0,0,0,,这样就能在单元中存储1或者0
Dialogue: 0,0:16:21.32,0:16:25.06,ENG,,0,0,0,,You can read out the stored value by applying a small voltage.
Dialogue: 0,0:16:21.32,0:16:25.06,CHN,,0,0,0,,施加较小的电压就能读出存储的值
Dialogue: 0,0:16:25.06,0:16:26.97,ENG,,0,0,0,,If there are electrons on the floating gate,
Dialogue: 0,0:16:25.06,0:16:26.97,CHN,,0,0,0,,如果有电子在悬浮门中
Dialogue: 0,0:16:26.97,0:16:29.58,ENG,,0,0,0,,no current flows, so that's a zero.
Dialogue: 0,0:16:26.97,0:16:29.58,CHN,,0,0,0,,单元中没有电流流过 代表0
Dialogue: 0,0:16:29.58,0:16:30.94,ENG,,0,0,0,,If there aren't electrons,
Dialogue: 0,0:16:29.58,0:16:30.94,CHN,,0,0,0,,如果悬浮门中没有电子
Dialogue: 0,0:16:30.94,0:16:33.92,ENG,,0,0,0,,then current does flow, and that's a one.
Dialogue: 0,0:16:30.94,0:16:33.92,CHN,,0,0,0,,则电流可以流过 这代表1
Dialogue: 0,0:16:33.92,0:16:35.03,ENG,,0,0,0,,Now Mythic's idea is
Dialogue: 0,0:16:33.92,0:16:35.03,CHN,,0,0,0,,现在 Mythic 的想法是
Dialogue: 0,0:16:35.03,0:16:37.74,ENG,,0,0,0,,to use these cells not as on/off switches,
Dialogue: 0,0:16:35.03,0:16:37.74,CHN,,0,0,0,,把这些单元用作可变电阻
Dialogue: 0,0:16:37.74,0:16:39.84,ENG,,0,0,0,,but as variable resistors.
Dialogue: 0,0:16:37.74,0:16:39.84,CHN,,0,0,0,,而不是开关
Dialogue: 0,0:16:39.84,0:16:40.69,ENG,,0,0,0,,They do this
Dialogue: 0,0:16:39.84,0:16:40.69,CHN,,0,0,0,,他们在每个悬浮门中
Dialogue: 0,0:16:40.69,0:16:43.93,ENG,,0,0,0,,by putting a specific number of electrons on each floating gate,
Dialogue: 0,0:16:40.69,0:16:43.93,CHN,,0,0,0,,放置指定数量的电子
Dialogue: 0,0:16:43.93,0:16:45.67,ENG,,0,0,0,,instead of all or nothing.
Dialogue: 0,0:16:43.93,0:16:45.67,CHN,,0,0,0,,而不是非空即满
Dialogue: 0,0:16:45.67,0:16:47.33,ENG,,0,0,0,,The greater the number of electrons,
Dialogue: 0,0:16:45.67,0:16:47.33,CHN,,0,0,0,,悬浮门中电子越多
Dialogue: 0,0:16:47.33,0:16:49.81,ENG,,0,0,0,,the higher the resistance of the channel.
Dialogue: 0,0:16:47.33,0:16:49.81,CHN,,0,0,0,,通道中的电阻就越高
Dialogue: 0,0:16:49.81,0:16:52.00,ENG,,0,0,0,,When you later apply a small voltage,
Dialogue: 0,0:16:49.81,0:16:52.00,CHN,,0,0,0,,如果之后在其上施加较小的电压
Dialogue: 0,0:16:52.00,0:16:55.72,ENG,,0,0,0,,the current that flows is equal to V over R.
Dialogue: 0,0:16:52.00,0:16:55.72,CHN,,0,0,0,,电流就等于 V 除以 R
Dialogue: 0,0:16:55.72,0:16:59.25,ENG,,0,0,0,,But you can also think of this as voltage times conductance,
Dialogue: 0,0:16:55.72,0:16:59.25,CHN,,0,0,0,,但你也可以把它视作电压与电导的乘积
Dialogue: 0,0:16:59.25,0:17:02.51,ENG,,0,0,0,,where conductance is just the reciprocal of resistance.
Dialogue: 0,0:16:59.25,0:17:02.51,CHN,,0,0,0,,其中电导是电阻的倒数
Dialogue: 0,0:17:02.51,0:17:04.47,ENG,,0,0,0,,So a single flash cell can be used
Dialogue: 0,0:17:02.51,0:17:04.47,CHN,,0,0,0,,于是一个单独的闪存单元
Dialogue: 0,0:17:04.47,0:17:09.13,ENG,,0,0,0,,to multiply two values together, voltage times conductance.
Dialogue: 0,0:17:04.47,0:17:09.13,CHN,,0,0,0,,可以对两个值做乘法 即电压乘以电导
Dialogue: 0,0:17:09.13,0:17:11.86,ENG,,0,0,0,,So to use this to run an artificial neural network,
Dialogue: 0,0:17:09.13,0:17:11.86,CHN,,0,0,0,,所以要用这种方法运行一个人工神经网络
Dialogue: 0,0:17:11.86,0:17:14.75,ENG,,0,0,0,,well they first write all the weights to the flash cells
Dialogue: 0,0:17:11.86,0:17:14.75,CHN,,0,0,0,,他们会首先将所有的权重作为电导
Dialogue: 0,0:17:14.75,0:17:16.87,ENG,,0,0,0,,as each cell's conductance.
Dialogue: 0,0:17:14.75,0:17:16.87,CHN,,0,0,0,,写入闪存
Dialogue: 0,0:17:16.87,0:17:19.27,ENG,,0,0,0,,Then, they input the activation values
Dialogue: 0,0:17:16.87,0:17:19.27,CHN,,0,0,0,,然后 他们将激活值
Dialogue: 0,0:17:19.27,0:17:21.49,ENG,,0,0,0,,as the voltage on the cells.
Dialogue: 0,0:17:19.27,0:17:21.49,CHN,,0,0,0,,作为电压施加在单元两端
Dialogue: 0,0:17:21.49,0:17:23.63,ENG,,0,0,0,,And the resulting current is the product
Dialogue: 0,0:17:21.49,0:17:23.63,CHN,,0,0,0,,所得的结果就是
Dialogue: 0,0:17:23.63,0:17:25.50,ENG,,0,0,0,,of voltage times conductance,
Dialogue: 0,0:17:23.63,0:17:25.50,CHN,,0,0,0,,电压和电导的乘积
Dialogue: 0,0:17:25.50,0:17:28.30,ENG,,0,0,0,,which is activation times weight.
Dialogue: 0,0:17:25.50,0:17:28.30,CHN,,0,0,0,,也即激活函数的激活值乘以权重
Dialogue: 0,0:17:28.30,0:17:30.86,ENG,,0,0,0,,The cells are wired together in such a way
Dialogue: 0,0:17:28.30,0:17:30.86,CHN,,0,0,0,,这些单元相互连接
Dialogue: 0,0:17:30.86,0:17:34.00,ENG,,0,0,0,,that the current from each multiplication adds together,
Dialogue: 0,0:17:30.86,0:17:34.00,CHN,,0,0,0,,可以对每处乘法的结果求和
Dialogue: 0,0:17:34.00,0:17:36.60,ENG,,0,0,0,,completing the matrix multiplication.
Dialogue: 0,0:17:34.00,0:17:36.60,CHN,,0,0,0,,这样就实现了矩阵乘法
Dialogue: 0,0:17:39.04,0:17:40.92,ENG,,0,0,0,,- So this is our first product.
Dialogue: 0,0:17:39.04,0:17:40.92,CHN,,0,0,0,,- 这是我们的第一件产品
Dialogue: 0,0:17:40.92,0:17:45.86,ENG,,0,0,0,,This can do 25 trillion math operations per second.
Dialogue: 0,0:17:40.92,0:17:45.86,CHN,,0,0,0,,它每秒能完成 25 万亿次数学运算
Dialogue: 0,0:17:45.86,0:17:47.04,ENG,,0,0,0,,- 25 trillion.
Dialogue: 0,0:17:45.86,0:17:47.04,CHN,,0,0,0,,- 25 万亿次
Dialogue: 0,0:17:47.04,0:17:49.40,ENG,,0,0,0,,- Yep, 25 trillion math operations per second,
Dialogue: 0,0:17:47.04,0:17:49.40,CHN,,0,0,0,,- 没错 每秒25万亿次数学运算
Dialogue: 0,0:17:49.40,0:17:50.54,ENG,,0,0,0,,in this little chip here,
Dialogue: 0,0:17:49.40,0:17:50.54,CHN,,0,0,0,,这块小小芯片
Dialogue: 0,0:17:50.54,0:17:52.62,ENG,,0,0,0,,burning about three watts of power.
Dialogue: 0,0:17:50.54,0:17:52.62,CHN,,0,0,0,,功耗大约是 3 瓦特
Dialogue: 0,0:17:52.62,0:17:54.93,ENG,,0,0,0,,- How does it compare to a digital chip?
Dialogue: 0,0:17:52.62,0:17:54.93,CHN,,0,0,0,,- 它与数字芯片相比起来如何
Dialogue: 0,0:17:54.93,0:17:57.90,ENG,,0,0,0,,- The newer digital systems can do anywhere
Dialogue: 0,0:17:54.93,0:17:57.90,CHN,,0,0,0,,较新的数字元件能在一秒内
Dialogue: 0,0:17:57.90,0:18:00.68,ENG,,0,0,0,,from 25 to 100 trillion operations per second,
Dialogue: 0,0:17:57.90,0:18:00.68,CHN,,0,0,0,,完成 25 万亿到 100 万亿次数学运算
Dialogue: 0,0:18:00.68,0:18:02.78,ENG,,0,0,0,,but they are big, thousand-dollar systems
Dialogue: 0,0:18:00.68,0:18:02.78,CHN,,0,0,0,,但是那都是些庞大而昂贵的系统 价值数千美元
Dialogue: 0,0:18:02.78,0:18:06.59,ENG,,0,0,0,,that are spitting out 50 to 100 watts of power.
Dialogue: 0,0:18:02.78,0:18:06.59,CHN,,0,0,0,,它们的功率在 50 到 100 瓦特
Dialogue: 0,0:18:06.59,0:18:07.68,ENG,,0,0,0,,- Obviously this isn't
Dialogue: 0,0:18:06.59,0:18:07.68,CHN,,0,0,0,,- 显然
Dialogue: 0,0:18:07.68,0:18:09.46,ENG,,0,0,0,,like an apples apples comparison, right?
Dialogue: 0,0:18:07.68,0:18:09.46,CHN,,0,0,0,,这两者是没法相比的 对吧
Dialogue: 0,0:18:09.46,0:18:10.58,ENG,,0,0,0,,- No, it's not apples to apples.
Dialogue: 0,0:18:09.46,0:18:10.58,CHN,,0,0,0,,- 不 完全没法比
Dialogue: 0,0:18:10.58,0:18:13.66,ENG,,0,0,0,,I mean, training those algorithms,
Dialogue: 0,0:18:10.58,0:18:13.66,CHN,,0,0,0,,我的意思是
Dialogue: 0,0:18:13.66,0:18:15.51,ENG,,0,0,0,,you need big hardware like this.
Dialogue: 0,0:18:13.66,0:18:15.51,CHN,,0,0,0,,你需要这些大家伙来训练神经网络
Dialogue: 0,0:18:15.51,0:18:17.61,ENG,,0,0,0,,You can just do all sorts of stuff on the GPU,
Dialogue: 0,0:18:15.51,0:18:17.61,CHN,,0,0,0,,你可以在 GPU 上完成所有这些事
Dialogue: 0,0:18:17.61,0:18:20.36,ENG,,0,0,0,,but if you specifically are doing AI workloads
Dialogue: 0,0:18:17.61,0:18:20.36,CHN,,0,0,0,,但是如果你想应用某个 AI
Dialogue: 0,0:18:20.36,0:18:22.72,ENG,,0,0,0,,and you wanna deploy 'em, you could use this instead.
Dialogue: 0,0:18:20.36,0:18:22.72,CHN,,0,0,0,,并且部署它们 就可以使用芯片来替代 GPU
Dialogue: 0,0:18:22.72,0:18:25.17,ENG,,0,0,0,,You can imagine them in security cameras,
Dialogue: 0,0:18:22.72,0:18:25.17,CHN,,0,0,0,,不难想象它们在安保摄像头
Dialogue: 0,0:18:25.17,0:18:26.69,ENG,,0,0,0,,autonomous systems,
Dialogue: 0,0:18:25.17,0:18:26.69,CHN,,0,0,0,,自动化系统
Dialogue: 0,0:18:26.69,0:18:29.12,ENG,,0,0,0,,inspection equipment for manufacturing.
Dialogue: 0,0:18:26.69,0:18:29.12,CHN,,0,0,0,,流水线检测中的应用
Dialogue: 0,0:18:29.12,0:18:30.88,ENG,,0,0,0,,Every time they make a Frito-Lay chip,
Dialogue: 0,0:18:29.12,0:18:30.88,CHN,,0,0,0,,每生产一包乐事薯片
Dialogue: 0,0:18:30.88,0:18:32.20,ENG,,0,0,0,,they inspect it with a camera,
Dialogue: 0,0:18:30.88,0:18:32.20,CHN,,0,0,0,,他们会用一个摄像头检查它
Dialogue: 0,0:18:32.20,0:18:36.17,ENG,,0,0,0,,and the bad Fritos get blown off of the conveyor belt.
Dialogue: 0,0:18:32.20,0:18:36.17,CHN,,0,0,0,,然后移除传送带上的次品
Dialogue: 0,0:18:36.17,0:18:37.75,ENG,,0,0,0,,But they're using artificial intelligence
Dialogue: 0,0:18:36.17,0:18:37.75,CHN,,0,0,0,,他们是在用人工智能
Dialogue: 0,0:18:37.75,0:18:40.41,ENG,,0,0,0,,to spot which Fritos are good and bad.
Dialogue: 0,0:18:37.75,0:18:40.41,CHN,,0,0,0,,检查产品的质量
Dialogue: 0,0:18:40.41,0:18:41.35,ENG,,0,0,0,,- Some have proposed
Dialogue: 0,0:18:40.41,0:18:41.35,CHN,,0,0,0,,有人提议在智能家居音箱中采用模拟芯片
Dialogue: 0,0:18:41.35,0:18:43.92,ENG,,0,0,0,,using analog circuitry in smart home speakers,
Dialogue: 0,0:18:41.35,0:18:43.92,CHN,,0,0,0,,有人提议在智能家居音箱中采用模拟芯片
Dialogue: 0,0:18:43.92,0:18:47.65,ENG,,0,0,0,,solely to listen for the wake word, like Alexa or Siri.
Dialogue: 0,0:18:43.92,0:18:47.65,CHN,,0,0,0,,来单独监听唤醒词 像是 Alexa 或者 Siri
Dialogue: 0,0:18:47.65,0:18:48.93,ENG,,0,0,0,,They would use a lot less power
Dialogue: 0,0:18:47.65,0:18:48.93,CHN,,0,0,0,,这样可以降低功耗
Dialogue: 0,0:18:48.93,0:18:50.92,ENG,,0,0,0,,and be able to quickly and reliably turn on
Dialogue: 0,0:18:48.93,0:18:50.92,CHN,,0,0,0,,同时能够稳定而快速地启动
Dialogue: 0,0:18:50.92,0:18:53.31,ENG,,0,0,0,,the digital circuitry of the device.
Dialogue: 0,0:18:50.92,0:18:53.31,CHN,,0,0,0,,设备的数字电路
Dialogue: 0,0:18:53.31,0:18:56.42,ENG,,0,0,0,,But you still have to deal with the challenges of analog.
Dialogue: 0,0:18:53.31,0:18:56.42,CHN,,0,0,0,,但是模拟信号中的挑战仍然亟待解决
Dialogue: 0,0:18:56.42,0:18:58.12,ENG,,0,0,0,,- So for one of the popular networks,
Dialogue: 0,0:18:56.42,0:18:58.12,CHN,,0,0,0,,- 对于一个最为流行的神经网络
Dialogue: 0,0:18:58.12,0:19:02.67,ENG,,0,0,0,,there would be 50 sequences of matrix multiplies that you're doing.
Dialogue: 0,0:18:58.12,0:19:02.67,CHN,,0,0,0,,其中有 50 个序列的矩阵乘法需要完成
Dialogue: 0,0:19:02.67,0:19:05.04,ENG,,0,0,0,,Now, if you did that entirely in the analog domain,
Dialogue: 0,0:19:02.67,0:19:05.04,CHN,,0,0,0,,现在如果你完全在模拟域进行计算
Dialogue: 0,0:19:05.04,0:19:06.34,ENG,,0,0,0,,by the time it gets to the output,
Dialogue: 0,0:19:05.04,0:19:06.34,CHN,,0,0,0,,在输出时
Dialogue: 0,0:19:06.34,0:19:10.26,ENG,,0,0,0,,it's just so distorted that you don't have any result at all.
Dialogue: 0,0:19:06.34,0:19:10.26,CHN,,0,0,0,,信号会极度失真以至于完全得不到结果
Dialogue: 0,0:19:10.26,0:19:12.13,ENG,,0,0,0,,So you convert it from the analog domain,
Dialogue: 0,0:19:10.26,0:19:12.13,CHN,,0,0,0,,所以需要把信号从模拟域
Dialogue: 0,0:19:12.13,0:19:14.09,ENG,,0,0,0,,back to the digital domain,
Dialogue: 0,0:19:12.13,0:19:14.09,CHN,,0,0,0,,转换回数字域
Dialogue: 0,0:19:14.09,0:19:15.97,ENG,,0,0,0,,send it to the next processing block,
Dialogue: 0,0:19:14.09,0:19:15.97,CHN,,0,0,0,,将它送入下一个处理模块
Dialogue: 0,0:19:15.97,0:19:18.36,ENG,,0,0,0,,and then you convert it into the analog domain again.
Dialogue: 0,0:19:15.97,0:19:18.36,CHN,,0,0,0,,然后再把它转换到模拟域
Dialogue: 0,0:19:18.36,0:19:20.49,ENG,,0,0,0,,And that allows you to preserve the signal.
Dialogue: 0,0:19:18.36,0:19:20.49,CHN,,0,0,0,,这样就可以避免信号失真
Dialogue: 0,0:19:20.49,0:19:22.64,ENG,,0,0,0,,- You know, when Rosenblatt was first setting
Dialogue: 0,0:19:20.49,0:19:22.64,CHN,,0,0,0,,所以  当罗森布拉特首次搭建感知机的时候
Dialogue: 0,0:19:22.64,0:19:23.75,ENG,,0,0,0,,up his perceptron,
Dialogue: 0,0:19:22.64,0:19:23.75,CHN,,0,0,0,,所以  当罗森布拉特首次搭建感知机的时候
Dialogue: 0,0:19:23.75,0:19:26.70,ENG,,0,0,0,,he used a digital IBM computer.
Dialogue: 0,0:19:23.75,0:19:26.70,CHN,,0,0,0,,用的还是IBM的数字计算机
Dialogue: 0,0:19:26.70,0:19:28.33,ENG,,0,0,0,,Finding it too slow,
Dialogue: 0,0:19:26.70,0:19:28.33,CHN,,0,0,0,,但发现它太慢了
Dialogue: 0,0:19:28.33,0:19:30.84,ENG,,0,0,0,,he built a custom analog computer,
Dialogue: 0,0:19:28.33,0:19:30.84,CHN,,0,0,0,,随后又搭建了定制的模拟计算机
Dialogue: 0,0:19:30.84,0:19:32.57,ENG,,0,0,0,,complete with variable resistors
Dialogue: 0,0:19:30.84,0:19:32.57,CHN,,0,0,0,,这回用的是变阻器和小电机来驱动
Dialogue: 0,0:19:32.57,0:19:35.29,ENG,,0,0,0,,and little motors to drive them.
Dialogue: 0,0:19:32.57,0:19:35.29,CHN,,0,0,0,,这回用的是变阻器和小电机来驱动
Dialogue: 0,0:19:35.29,0:19:37.81,ENG,,0,0,0,,Ultimately, his idea of neural networks
Dialogue: 0,0:19:35.29,0:19:37.81,CHN,,0,0,0,,最终  人们发现他神经网络的想法是正确的
Dialogue: 0,0:19:37.81,0:19:39.40,ENG,,0,0,0,,turned out to be right.
Dialogue: 0,0:19:37.81,0:19:39.40,CHN,,0,0,0,,最终  人们发现他神经网络的想法是正确的
Dialogue: 0,0:19:39.40,0:19:42.37,ENG,,0,0,0,,Maybe he was right about analog, too.
Dialogue: 0,0:19:39.40,0:19:42.37,CHN,,0,0,0,,也许他的模拟计算机也会是正确的
Dialogue: 0,0:19:43.25,0:19:45.56,ENG,,0,0,0,,Now, I can't say whether analog computers
Dialogue: 0,0:19:43.25,0:19:45.56,CHN,,0,0,0,,现在  我也不确定模拟计算机能否走上
Dialogue: 0,0:19:45.56,0:19:48.69,ENG,,0,0,0,,will take off the way digital did last century,
Dialogue: 0,0:19:45.56,0:19:48.69,CHN,,0,0,0,,数字计算机在上个世纪的光辉道路
Dialogue: 0,0:19:48.69,0:19:51.36,ENG,,0,0,0,,but they do seem to be better suited
Dialogue: 0,0:19:48.69,0:19:51.36,CHN,,0,0,0,,但是有些时候  模拟计算机更适合
Dialogue: 0,0:19:51.36,0:19:55.21,ENG,,0,0,0,,to a lot of the tasks that we want computers to perform today,
Dialogue: 0,0:19:51.36,0:19:55.21,CHN,,0,0,0,,处理人们现在需要的计算任务
Dialogue: 0,0:19:55.21,0:19:56.24,ENG,,0,0,0,,which is a little bit funny
Dialogue: 0,0:19:55.21,0:19:56.24,CHN,,0,0,0,,有趣的是
Dialogue: 0,0:19:56.24,0:19:58.08,ENG,,0,0,0,,because I always thought of digital
Dialogue: 0,0:19:56.24,0:19:58.08,CHN,,0,0,0,,我总以为数字技术是处理信息的最佳选项
Dialogue: 0,0:19:58.08,0:20:01.53,ENG,,0,0,0,,as the optimal way of processing information.
Dialogue: 0,0:19:58.08,0:20:01.53,CHN,,0,0,0,,我总以为数字技术是处理信息的最佳选项
Dialogue: 0,0:20:01.53,0:20:03.77,ENG,,0,0,0,,Everything from music to pictures,
Dialogue: 0,0:20:01.53,0:20:03.77,CHN,,0,0,0,,从音乐到照片再到视频
Dialogue: 0,0:20:03.77,0:20:07.86,ENG,,0,0,0,,to video has all gone digital in the last 50 years.
Dialogue: 0,0:20:03.77,0:20:07.86,CHN,,0,0,0,,我们身边的一切  在过去的50年里都变成了数字媒体
Dialogue: 0,0:20:07.86,0:20:09.67,ENG,,0,0,0,,But maybe in a 100 years,
Dialogue: 0,0:20:07.86,0:20:09.67,CHN,,0,0,0,,但也许再过一百年
Dialogue: 0,0:20:09.67,0:20:11.44,ENG,,0,0,0,,we will look back on digital,
Dialogue: 0,0:20:09.67,0:20:11.44,CHN,,0,0,0,,我们回望数字技术的时候
Dialogue: 0,0:20:11.44,0:20:15.06,ENG,,0,0,0,,not as the end point of information technology,
Dialogue: 0,0:20:11.44,0:20:15.06,CHN,,0,0,0,,发现它不是信息技术的终点
Dialogue: 0,0:20:15.06,0:20:17.32,ENG,,0,0,0,,but as a starting point.
Dialogue: 0,0:20:15.06,0:20:17.32,CHN,,0,0,0,,而是起点
Dialogue: 0,0:20:17.32,0:20:21.93,ENG,,0,0,0,,Our brains are digital in that a neuron either fires or it doesn't,
Dialogue: 0,0:20:17.32,0:20:21.93,CHN,,0,0,0,,神经元信号的发射（1）与不发射（0）代表的大脑是数字式的
Dialogue: 0,0:20:21.93,0:20:24.04,ENG,,0,0,0,,but they're also analog
Dialogue: 0,0:20:21.93,0:20:24.04,CHN,,0,0,0,,但是他们也是模拟的
Dialogue: 0,0:20:24.04,0:20:28.22,ENG,,0,0,0,,in that thinking takes place everywhere, all at once.
Dialogue: 0,0:20:24.04,0:20:28.22,CHN,,0,0,0,,我们思考的过程是同时发生在大脑各处的
Dialogue: 0,0:20:28.22,0:20:32.49,ENG,,0,0,0,,So maybe what we need to achieve true artificial intelligence,
Dialogue: 0,0:20:28.22,0:20:32.49,CHN,,0,0,0,,因此  我们要想实现真正的人工智能
Dialogue: 0,0:20:32.49,0:20:34.68,ENG,,0,0,0,,machines that think like us,
Dialogue: 0,0:20:32.49,0:20:34.68,CHN,,0,0,0,,造出像人类一样思考的机器
Dialogue: 0,0:20:34.68,0:20:37.13,ENG,,0,0,0,,is the power of analog.
Dialogue: 0,0:20:34.68,0:20:37.13,CHN,,0,0,0,,模拟的力量也许真的不可或缺
